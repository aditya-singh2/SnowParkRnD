{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef82854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys, os\n",
    "from snowflake.snowpark.session import Session\n",
    "import json\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "def create_stage(session, stage_name=\"insight_exp\"):\n",
    "    try:\n",
    "        session.sql(f\"create or replace stage {stage_name}\").collect()\n",
    "        return f\"@{stage_name}\"\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake stage\", ex)\n",
    "        log.error(\"Error while creating snowflake stage\", ex)\n",
    "        raise ex\n",
    "\n",
    "\n",
    "# Method to be registered as SF Stored Procedure \n",
    "def run_experiment(session: Session, exp_data: dict) -> list:\n",
    "    # Imports\n",
    "    from snowflake.ml.modeling.pipeline import Pipeline\n",
    "    from snowflake.ml.modeling.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "    from snowflake.snowpark.functions import col\n",
    "    from snowflake.snowpark.types import StringType\n",
    "    from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "    from fosforml import register_model\n",
    "    import importlib, json, os\n",
    "\n",
    "\n",
    "    # variable for holding logs\n",
    "    logs = []\n",
    "    \n",
    "    # function for accumulating logs\n",
    "    def log_message(level: str, message: str):\n",
    "        logs.append(f\"{level}: {message}\")\n",
    "    \n",
    "    \n",
    "    # Functions used in stored proc\n",
    "    def apply_data_cleansing(df):\n",
    "        \"\"\"\n",
    "        Method handles null values in snowpark dataframe.\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        :returns:\n",
    "        df_cleaned: dataframe after null handling\n",
    "        \"\"\"\n",
    "        # fillna - Unknown and 0\n",
    "        schema_fields = df.schema.fields\n",
    "        fill_values = {field.name: \"UNKNOWN\" if isinstance(field.datatype, StringType) else 0 for field in schema_fields}\n",
    "        df_cleaned = df.fillna(fill_values)\n",
    "        return df_cleaned\n",
    "\n",
    "\n",
    "    def get_feature_columns(df):\n",
    "        \"\"\"\n",
    "        Identifies the numerical and categorical features in dataset.\n",
    "        Identifies features for label encoding and one hot encoding\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        :returns:\n",
    "        categorical_features: list of non-numerical feature columns\n",
    "        numerical_features: list of numerical feature columns\n",
    "        le_column_features: list of feature label encoder columns\n",
    "        oh_column_features: list of feature one hot encoder columns\n",
    "        \"\"\"\n",
    "        schema_fields = df.schema.fields\n",
    "        features = df.columns\n",
    "        features.remove(exp_data.get(\"target_column\"))\n",
    "        df_schema = session.sql(f\"DESCRIBE TABLE {exp_data.get('dataset_name')}\").collect()\n",
    "        categorical_types = ['VARCHAR','CHAR','STRING','TEXT','BOOL']\n",
    "        categorical_features = []  \n",
    "        for row in df_schema:\n",
    "            for typ in categorical_types:\n",
    "                if typ in row['type']:\n",
    "                    categorical_features.append(row['name'])\n",
    "                    break\n",
    "        numerical_features = list(set(features) - set(categorical_features))\n",
    "        log_message(\"INFO\",f\"numerical_features:  {numerical_features}\")\n",
    "        log_message(\"INFO\",f\"categorical_features: {categorical_features}\")\n",
    "        \n",
    "        \n",
    "        #identify columns for labelencoding and onehotencoding   \n",
    "        le_column_features = categorical_features\n",
    "        oh_column_features = []\n",
    "        if len(categorical_features) >= 1:\n",
    "            log_message(\"INFO\",f\"{categorical_features} columns are non numeric in feature dataset, encoding required.\")\n",
    "            for column in categorical_features:\n",
    "                if df.select(df[column]).distinct().count() < 10:\n",
    "                    oh_column_features.append(column)\n",
    "            log_message(\"INFO\",f\"Columns identified to be encoded with label encoder: {le_column_features}\")\n",
    "            log_message(\"INFO\",f\"Columns identified to be encoded with one hot encoder: {oh_column_features}\")\n",
    "        return categorical_features, numerical_features, le_column_features, oh_column_features\n",
    "\n",
    "\n",
    "    def create_and_run_preprocessing(df, categorical_features, numerical_features, le_column_features, oh_column_features):\n",
    "        \"\"\"\n",
    "        Based on different features column input creates preprocessing steps and run it on input dataframe\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        categorical_features: list of non-numerical feature columns\n",
    "        numerical_features: list of numerical feature columns\n",
    "        le_column_features: list of feature label encoder columns\n",
    "        oh_column_features: list of feature one hot encoder columns\n",
    "        :returns:\n",
    "        df_train: preprocessed train dataset\n",
    "        df_test: preprocessed test dataset\n",
    "        \"\"\"\n",
    "        #pipeline steps \n",
    "        log_message(\"INFO\",\"Setting up preprocessing pipeline based on dataset\")\n",
    "        categorical_pp = {f'le_{column}':LabelEncoder(input_cols=column, output_cols=column) for column in le_column_features}\n",
    "        if len(oh_column_features)>0:\n",
    "            categorical_pp['oh_enc'] = OneHotEncoder(input_cols=oh_column_features, output_cols=oh_column_features, handle_unknown='ignore')\n",
    "        numerical_pp = {\n",
    "            'scaler': MinMaxScaler(input_cols=numerical_features, output_cols=numerical_features)\n",
    "        }\n",
    "        steps = [(key, categorical_pp[key]) for key in categorical_pp if categorical_pp[key]!=[]] + \\\n",
    "        [(key, numerical_pp[key]) for key in numerical_pp if numerical_features!=[]]\n",
    "        log_message(\"INFO\",f\"Selected preprocesing steps: \\n{steps}\")   \n",
    "            \n",
    "        # Run preprocessing pipeline steps\n",
    "        log_message(\"INFO\",\"Running data preprocessing pipeline\") \n",
    "        df = Pipeline(steps=steps).fit(df).transform(df)\n",
    "        log_message(\"INFO\",f\"Transformed dataset: \\n {df.show()}\")\n",
    "        df_train, df_test = df.random_split(weights=[0.8, 0.2], seed=0)\n",
    "        for col_name in df_train.schema.names:\n",
    "            new_col = col_name.replace('.','_')\n",
    "            df_train = df_train.withColumnRenamed(col_name, new_col)\n",
    "        for col_name in df_test.schema.names:\n",
    "            new_col = col_name.replace('.','_')\n",
    "            df_test = df_test.withColumnRenamed(col_name, new_col)\n",
    "        return df_train, df_test\n",
    "\n",
    "\n",
    "    def run_estimator(df_train, df_test, input_cols):\n",
    "        \"\"\"\n",
    "        trains on df_train and creates model object for given algorithm/estimator.\n",
    "        runs prediction function of model object on test dataset\n",
    "        :param:\n",
    "        df_train: input training dataframe\n",
    "        df_test: input test dataframe\n",
    "        input_cols: list of input feature names\n",
    "        :returns:\n",
    "        model: trained model object\n",
    "        df_pred: output predict dataframe\n",
    "        \"\"\"\n",
    "        for algorithm in exp_data.get(\"algorithms\"):\n",
    "            algorithm = algorithm[\"algorithm_name\"].rsplit('.', 1)\n",
    "            module = importlib.import_module(algorithm[0])\n",
    "            log_message(\"INFO\",f\"----Creating {algorithm[1]} algorithm pipeline----\")\n",
    "            attr = getattr(module, algorithm[1])\n",
    "            pipe = Pipeline(steps=[(\"algorithm\", attr(input_cols=input_cols\n",
    "                                                  , label_cols=[exp_data.get(\"target_column\")]\n",
    "                                                  , output_cols=['PREDICTIONS']))]\n",
    "                   )\n",
    "    \n",
    "            # Fit the pipeline\n",
    "            log_message(\"INFO\",f\"Running model pipeline {algorithm[1]}\")\n",
    "            model = pipe.fit(df_train)\n",
    "     \n",
    "            # Test the model\n",
    "            log_message(\"INFO\",\"Running prediction on model with test dataset\")\n",
    "            df_pred = model.predict(df_test)\n",
    "            return model, df_pred, algorithm[1]\n",
    "\n",
    "\n",
    "    def register_model_using_fosforml(model, df_pred):\n",
    "        response = register_model(\n",
    "            model_obj=model,\n",
    "            session=session,\n",
    "            name=exp_data.get(\"experiment_name\"),\n",
    "            snowflake_df=df_pred,\n",
    "            dataset_name=_exp_data.get(\"dataset_name\"),\n",
    "            dataset_source=\"SnowflakeDataset\",\n",
    "            description=exp_data.get(\"description\",\"This is an experiment generated model\"),\n",
    "            flavour=\"snowflake\",\n",
    "            model_type=exp_data.get(\"algorithm_type\").lower(),\n",
    "            conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "            prediction_column=\"PREDICTIONS\",\n",
    "            source=\"Experiment\"\n",
    "        )\n",
    "        model_name = response.split(\" \")[1]\n",
    "        return model_name\n",
    "\n",
    "\n",
    "    try:\n",
    "        # loading experiment details ans setting up environment\n",
    "        log_message(\"INFO\",f\"loading experiment details ans setting up environment\")\n",
    "        os.environ[\"RUN_ID\"] = os.environ['run_id'] = exp_data.get(\"monitor_run_id\")\n",
    "        os.environ[\"algorithm_type\"] = exp_data.get(\"algorithm_type\")\n",
    "        # os.environ[\"algorithm\"] = exp_data.get(\"algorithm\")\n",
    "        os.environ[\"experiment_name\"] = exp_data.get(\"experiment_name\", \"\")\n",
    "        os.environ[\"experiment_id\"] = exp_data.get(\"experiment_id\")\n",
    "        os.environ[\"PROJECT_ID\"] = exp_data.get(\"project_id\")\n",
    "\n",
    "        # Reading dataset\n",
    "        log_message(\"INFO\",\"Reading dataset features\")\n",
    "        data = session.table(exp_data.get(\"dataset_name\"))\n",
    "\n",
    "        # fillna\n",
    "        log_message(\"INFO\",\"Handling nulls in dataset\")\n",
    "        data = apply_data_cleansing(data)\n",
    "\n",
    "        # Identify feature columns\n",
    "        log_message(\"INFO\",\"Identifying feature columns\")\n",
    "        categorical_features, numerical_features, le_column_features, oh_column_features = get_feature_columns(data)\n",
    "\n",
    "        # Based on feature, do preprocessing\n",
    "        data_train, data_test = create_and_run_preprocessing(data, categorical_features, numerical_features, le_column_features, oh_column_features)\n",
    "\n",
    "        # Run model training and prediction\n",
    "        input_cols = data_train.columns\n",
    "        input_cols.remove(exp_data.get(\"target_column\"))\n",
    "        model, data_pred, estimator = run_estimator(data_train, data_test, input_cols)\n",
    "\n",
    "        os.environ[\"algorithm\"] = estimator\n",
    "\n",
    "        # Register model on snowflake registry and generate metrices\n",
    "        log_message(\"INFO\", \"leveraging fosforml for registering model and evaluating metrices\")\n",
    "        model_name = register_model_using_fosforml(model, data_pred)\n",
    "        if model_name.startswith(\"'EXPERIMENT\"):\n",
    "              log_message(\"INFO\", f\"Model registration failed, {response}\")\n",
    "              raise Exception(f\"Model registration failed, {response}\")\n",
    "        log_message(\"INFO\", f\"Model registered on snowflake registry by name: {model_name}\")\n",
    "\n",
    "        return [{\"Execution Logs:\": \"\\n\".join(logs),\n",
    "                 \"experiment_name\":exp_data.get(\"experiment_name\"),\n",
    "                 \"algorithm_type\":exp_data.get(\"algorithm_type\"),\n",
    "                 \"algorithm\": estimator,\n",
    "                 \"dataset_name\": exp_data.get(\"dataset_name\"),\n",
    "                 \"target_column\": exp_data.get(\"target_column\"),\n",
    "                 \"run_status\": \"SUCCESS\",\n",
    "                 \"registry_model_name\": model_name}]\n",
    "    except Exception as ex:\n",
    "        log_message(\"ERROR\", f\"{repr(ex)}\")\n",
    "        raise ex(\"Execution Logs: \"+\"\\n\".join(logs))\n",
    "        \n",
    "        \n",
    "def create_sproc(session, stage, func_name=\"run_experiment\"):\n",
    "    try:\n",
    "        session.custom_package_usage_config = {\n",
    "                                               \"enabled\": True,\n",
    "                                               'force_push': True\n",
    "                                            }\n",
    "                                            \n",
    "        session.sproc.register(func=run_experiment,\n",
    "                               name=\"run_experiment\",\n",
    "                               imports=[(\"notebooks_api/experiments/recipe/snowpark_recipe.py\",\"notebooks_api.experiments.recipe.snowpark_recipe\")],\n",
    "                               packages=[\"fosforml\"],\n",
    "                               is_permanent=True,\n",
    "                               stage_location=stage,\n",
    "                               if_not_exists=True)\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake stored procedure\", ex)\n",
    "        log.error(\"Error while creating snowflake stored procedure\", ex)\n",
    "        raise ex \n",
    "\n",
    "    \n",
    "def initiate_sproc_process(session, sproc_name=\"run_experiment\"): #payload\n",
    "    stage = create_stage(session)\n",
    "    log.info(\"Creating stored procedure...\")\n",
    "    create_sproc(session, stage)\n",
    "    log.info(\"Stored procedure has been created successfully!\")\n",
    "    return \"Success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767f65e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
