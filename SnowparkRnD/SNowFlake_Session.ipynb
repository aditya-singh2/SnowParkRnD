{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606818fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting snowflake-snowpark-python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/68/b3496e1d59e8c8b302df7fa5c4ce851a687b8b8ddb92d6f6254bfce77009/snowflake_snowpark_python-1.17.0-py3-none-any.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting snowflake-ml-python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/59/baf64a782ef609514f6d1ad11735504267d04794097d8a29809c4b4b2a38/snowflake_ml_python-1.5.1-py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 53.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/6b/6600ac24725c7388255b2f5add93f91e58a5d7efaf4af244fdbcc11a541b/PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736kB)\n",
      "\u001b[K     |████████████████████████████████| 737kB 46.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting snowflake-connector-python<4.0.0,>=3.10.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/a8/02ea1064d444d63cc8e97d8a7f4aa7606278ebe886513ea07b9d296caf8b/snowflake_connector_python-3.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5MB 51.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions<5.0.0,>=4.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/01/f3/936e209267d6ef7510322191003885de524fc48d1b43269810cd589ceaf5/typing_extensions-4.11.0-py3-none-any.whl\n",
      "Collecting cloudpickle!=2.1.0,!=2.2.0,<=2.2.1,>=1.6.0; python_version < \"3.11\"\n",
      "  Downloading https://files.pythonhosted.org/packages/15/80/44286939ca215e88fa827b2aeb6fa3fd2b4a7af322485c7170d6f9fd96e0/cloudpickle-2.2.1-py3-none-any.whl\n",
      "Collecting setuptools>=40.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/88/70c5767a0e43eb4451c2200f07d042a4bcd7639276003a9c54a68cfcc1f8/setuptools-70.0.0-py3-none-any.whl (863kB)\n",
      "\u001b[K     |██████████████████████████████  | 808kB 51.1MB/s eta 0:00:01^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install snowflake-snowpark-python snowflake-ml-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79433dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling scikit-learn-1.4.1.post1:\n",
      "  Successfully uninstalled scikit-learn-1.4.1.post1\n",
      "Collecting scikit-learn==1.0.2\n",
      "  Using cached https://files.pythonhosted.org/packages/57/aa/483fbe6b5314bce2d49801e6cec1f2139a9c220d0d51494788fff47233b3/scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl\n",
      "Collecting numpy>=1.14.6\n",
      "  Using cached https://files.pythonhosted.org/packages/54/30/c2a907b9443cf42b90c17ad10c1e8fa801975f01cb9764f3f8eb8aea638b/numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "Collecting joblib>=0.11\n",
      "  Using cached https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl\n",
      "Collecting scipy>=1.1.0\n",
      "  Using cached https://files.pythonhosted.org/packages/35/f5/d0ad1a96f80962ba65e2ce1de6a1e59edecd1f0a7b55990ed208848012e0/scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "\u001b[31mERROR: mosaic-utils 1.0.2 has requirement scikit-learn==1.2.1; python_version >= \"3.8\", but you'll have scikit-learn 1.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mlflow 2.6.0 has requirement pytz<2024, but you'll have pytz 2024.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.5.1 has requirement scikit-learn<1.4,>=1.2.1, but you'll have scikit-learn 1.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.5.1 has requirement xgboost<2,>=1.7.3, but you'll have xgboost 2.0.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement Flask==2.1.1; python_version >= \"3.7\", but you'll have flask 2.3.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement itsdangerous==2.0.1, but you'll have itsdangerous 2.1.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement Jinja2==3.0.3, but you'll have jinja2 3.1.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement matplotlib==3.6.0; python_version >= \"3.8\", but you'll have matplotlib 3.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-client 1.0.0 has requirement matplotlib==3.1.1, but you'll have matplotlib 3.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "Successfully installed joblib-1.4.2 numpy-1.26.4 scikit-learn-1.0.2 scipy-1.13.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib-1.4.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/sklearn already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn-1.0.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy-1.26.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/joblib already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scikit_learn.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/scipy-1.13.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl-3.5.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/threadpoolctl.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall clou -y \n",
    "!pip install scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838fd974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.snowpark.types import StructType, StructField, DoubleType, StringType\n",
    "import snowflake.snowpark.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3a45ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_parameters = {\n",
    "    \"account\": \"ug94937.us-east4.gcp\",\n",
    "    \"user\": \"ADITYASINGH\",\n",
    "    \"password\": \"Enlightme#2024\",\n",
    "    \"role\": \"ADITYASINGH\",  # optional\n",
    "    \"warehouse\": \"FOSFOR_INSIGHT_WH\",  # optional\n",
    "#     \"authenticator\": \"externalbrowser\", # optional\n",
    "    \"database\": \"FIRST_DB\",  # optional\n",
    "    \"schema\": \"PUBLIC\",  # optional\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9175c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection Established with the following parameters:\n",
      "User                        : ADITYASINGH\n",
      "Role                        : \"ADITYASINGH\"\n",
      "Database                    : \"FIRST_DB\"\n",
      "Schema                      : \"PUBLIC\"\n",
      "Warehouse                   : \"FOSFOR_INSIGHT_WH\"\n",
      "Snowflake version           : 8.20.0\n",
      "Snowpark for Python version : 1.17.0\n"
     ]
    }
   ],
   "source": [
    "# Make a Snowpark Connection\n",
    "\n",
    "################################################################################################################\n",
    "#  You can also use the SnowSQL Client to configure your connection params:\n",
    "#  https://docs.snowflake.com/en/user-guide/snowsql-install-config.html\n",
    "#\n",
    "#  >>> from snowflake.ml.utils import connection_params\n",
    "#  >>> session = Session.builder.configs(connection_params.SnowflakeLoginOptions()\n",
    "#  >>> ).create()   \n",
    "#\n",
    "#  NOTE: If you have named connection params then specify the connection name\n",
    "#  Example:\n",
    "#  \n",
    "#  >>> session = Session.builder.configs(\n",
    "#  >>> connection_params.SnowflakeLoginOptions(connection_name='connections.snowml')\n",
    "#  >>> ).create()\n",
    "#\n",
    "#################################################################################################################\n",
    "\n",
    "# Edit the connection.json before creating the session object below\n",
    "# Create Snowflake Session object\n",
    "# connection_parameters = json.load(open('connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('\\nConnection Established with the following parameters:')\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c644b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from snowflake.ml.modeling.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Create a session with your preferred method\n",
    "# session =\n",
    "\n",
    "NUMERICAL_COLS = [\"X1\", \"X2\", \"X3\"]\n",
    "CATEGORICAL_COLS = [\"C1\", \"C2\", \"C3\"]\n",
    "FEATURE_COLS = NUMERICAL_COLS + CATEGORICAL_COLS\n",
    "CATEGORICAL_OUTPUT_COLS = [\"C1_OUT\", \"C2_OUT\", \"C3_OUT\"]\n",
    "FEATURE_OUTPUT_COLS = [\"X1_FEAT_OUT\", \"X2_FEAT_OUT\", \"X3_FEAT_OUT\", \"C1_FEAT_OUT\", \"C2_FEAT_OUT\", \"C3_FEAT_OUT\"]\n",
    "\n",
    "# Create a dataset with numerical and categorical features\n",
    "X, _ = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=3,\n",
    "    noise=0.1,\n",
    "    random_state=0,\n",
    ")\n",
    "X = pd.DataFrame(X, columns=NUMERICAL_COLS)\n",
    "\n",
    "def generate_random_string(length):\n",
    "    return \"\".join(random.choices(string.ascii_uppercase, k=length))\n",
    "\n",
    "categorical_feature_length = 2\n",
    "categorical_features = {}\n",
    "for c in CATEGORICAL_COLS:\n",
    "    categorical_column = [generate_random_string(categorical_feature_length) for _ in range(X.shape[0])]\n",
    "    categorical_features[c] = categorical_column\n",
    "\n",
    "X = X.assign(**categorical_features)\n",
    "\n",
    "features_df = session.create_dataframe(X)\n",
    "\n",
    "# Fit a pipeline with OrdinalEncoder and MinMaxScaler on Snowflake\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"OE\",\n",
    "            OrdinalEncoder(\n",
    "                input_cols=CATEGORICAL_COLS,\n",
    "                output_cols=CATEGORICAL_OUTPUT_COLS,\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            \"MMS\",\n",
    "            MinMaxScaler(\n",
    "                input_cols=NUMERICAL_COLS + CATEGORICAL_OUTPUT_COLS,\n",
    "                output_cols=FEATURE_OUTPUT_COLS,\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline.fit(features_df)\n",
    "\n",
    "# Use the pipeline to transform a dataset.\n",
    "result = pipeline.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51e14469",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "(0000) Package cloudpickle==3.0.0 is not supported in snowflake conda channel for python runtime 3.8.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/ml/_internal/telemetry.py:368\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/ml/modeling/framework/base.py:436\u001b[0m, in \u001b[0;36mBaseEstimator.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(ds, data_source\u001b[38;5;241m.\u001b[39mDataSource) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_sources)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/ml/modeling/xgboost/xgb_classifier.py:471\u001b[0m, in \u001b[0;36mXGBClassifier._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    460\u001b[0m model_trainer \u001b[38;5;241m=\u001b[39m ModelTrainerBuilder\u001b[38;5;241m.\u001b[39mbuild(\n\u001b[1;32m    461\u001b[0m     estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sklearn_object,\n\u001b[1;32m    462\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size,\n\u001b[1;32m    470\u001b[0m )\n\u001b[0;32m--> 471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sklearn_object \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_fitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py:683\u001b[0m, in \u001b[0;36mSnowparkModelTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 683\u001b[0m     fit_wrapper_sproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_fit_wrapper_sproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py:289\u001b[0m, in \u001b[0;36mSnowparkModelTrainer._get_fit_wrapper_sproc\u001b[0;34m(self, statement_params)\u001b[0m\n\u001b[1;32m    287\u001b[0m fit_sproc_name \u001b[38;5;241m=\u001b[39m random_name_for_temp_object(TempObjectType\u001b[38;5;241m.\u001b[39mPROCEDURE)\n\u001b[0;32m--> 289\u001b[0m relaxed_dependencies \u001b[38;5;241m=\u001b[39m \u001b[43mpkg_version_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_valid_pkg_versions_supported_in_snowflake_conda_channel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpkg_versions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpkgDependencies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m fit_wrapper_sproc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39msproc\u001b[38;5;241m.\u001b[39mregister(\n\u001b[1;32m    294\u001b[0m     func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_fit_wrapper_sproc(model_spec\u001b[38;5;241m=\u001b[39mmodel_spec),\n\u001b[1;32m    295\u001b[0m     is_permanent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m     statement_params\u001b[38;5;241m=\u001b[39mstatement_params,\n\u001b[1;32m    301\u001b[0m )\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/ml/_internal/utils/pkg_version_utils.py:31\u001b[0m, in \u001b[0;36mget_valid_pkg_versions_supported_in_snowflake_conda_channel\u001b[0;34m(pkg_versions, session, subproject)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_valid_pkg_versions_supported_in_snowflake_conda_channel_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_versions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubproject\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/ml/_internal/utils/pkg_version_utils.py:75\u001b[0m, in \u001b[0;36m_get_valid_pkg_versions_supported_in_snowflake_conda_channel_async\u001b[0;34m(pkg_versions, session, subproject)\u001b[0m\n\u001b[1;32m     73\u001b[0m         cache[pkg_version] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m pkg_version_conda_list \u001b[38;5;241m=\u001b[39m \u001b[43m_get_conda_packages_and_emit_warnings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_versions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pkg_version_conda_list\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/ml/_internal/utils/pkg_version_utils.py:133\u001b[0m, in \u001b[0;36m_get_conda_packages_and_emit_warnings\u001b[0;34m(pkg_versions)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpkg_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported in snowflake conda channel for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython runtime \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_RUNTIME_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         )\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Package cloudpickle==3.0.0 is not supported in snowflake conda channel for python runtime 3.8.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train an XGBoost model on snowflake.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m xgboost_model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[1;32m     33\u001b[0m     input_cols\u001b[38;5;241m=\u001b[39mFEATURE_COLS,\n\u001b[1;32m     34\u001b[0m     label_cols\u001b[38;5;241m=\u001b[39mLABEL_COLS,\n\u001b[1;32m     35\u001b[0m     output_cols\u001b[38;5;241m=\u001b[39mOUTPUT_COLS\n\u001b[1;32m     36\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m \u001b[43mxgboost_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Use the model to make predictions.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m predictions \u001b[38;5;241m=\u001b[39m xgboost_model\u001b[38;5;241m.\u001b[39mpredict(features_df)\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/ml/_internal/telemetry.py:390\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m me\u001b[38;5;241m.\u001b[39moriginal_exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m me\u001b[38;5;241m.\u001b[39moriginal_exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m update_stmt_params_if_snowpark_df(res, statement_params)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: (0000) Package cloudpickle==3.0.0 is not supported in snowflake conda channel for python runtime 3.8."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from snowflake.ml.modeling.xgboost import XGBClassifier\n",
    "from snowflake.ml.utils.connection_params import SnowflakeLoginOptions\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Create a session with your preferred method\n",
    "# session =\n",
    "\n",
    "FEATURE_COLS = [\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\"]\n",
    "LABEL_COLS = [\"Y\"]\n",
    "OUTPUT_COLS = [\"PREDICTIONS\"]\n",
    "\n",
    "# Set up data.\n",
    "X, y = make_classification(\n",
    "    n_samples=40000,\n",
    "    n_features=6,\n",
    "    n_informative=4,\n",
    "    n_redundant=1,\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "X = pd.DataFrame(X, columns=FEATURE_COLS)\n",
    "y = pd.DataFrame(y, columns=LABEL_COLS)\n",
    "\n",
    "features_pandas = pd.concat([X, y], axis=1)\n",
    "features_df = session.create_dataframe(features_pandas)\n",
    "\n",
    "# Train an XGBoost model on snowflake.\n",
    "xgboost_model = XGBClassifier(\n",
    "    input_cols=FEATURE_COLS,\n",
    "    label_cols=LABEL_COLS,\n",
    "    output_cols=OUTPUT_COLS\n",
    ")\n",
    "\n",
    "xgboost_model.fit(features_df)\n",
    "\n",
    "# Use the model to make predictions.\n",
    "predictions = xgboost_model.predict(features_df)\n",
    "predictions[OUTPUT_COLS].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853c218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4334fe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloudpickle                3.0.0      \n",
      "\u001b[33mWARNING: The directory '/home/mosaic-ai/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -i cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06f845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
