{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fosforml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fosforio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! env | grep -i userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa851b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9345492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# project_id = str(uuid.uuid4())\n",
    "run_id = random.randint(100000, 999999)\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"fd0cf79c-8118-43a7-8f0b-059e8f78227a\".upper().replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FD0CF79C_8118_43A7_8F0B_059E8F78227A_exp_001_run_0001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_data = {\n",
    "    \"algorithm_category\": \"Snowparkml\",\n",
    "    \"algorithm_type\": \"Regression\",\n",
    "    \"algorithms\": [\n",
    "      {\n",
    "        \"algorithm_name\": \"snowflake.ml.modeling.ensemble.RandomForestRegressor\"\n",
    "      }\n",
    "    ],\n",
    "    \"dataset_name\": \"DIAMONDS\",\n",
    "    \"experiment_id\": \"600b9f90-f431-4590-83c7-f38fe138dedc\",\n",
    "    \"experiment_name\": \"New_Exper\",\n",
    "    \"monitor_run_id\": \"343356\",\n",
    "    \"project_id\": \"68bce134-6d7a-4e24-9591-86266438acf9\",\n",
    "    \"stored_procedure\": \"run_experiment\",\n",
    "    \"target_column\": \"PRICE\",\n",
    "    \"userId\": \"aditya1.singh2@fosfor.com\"\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_data = '''{\n",
    "            \"monitor_run_id\": \"16e3f179-bb70-4632-9a39-9a6fac4a5581\",\n",
    "            \"project_id\": \"test12345\",\n",
    "            \"experiment_id\" : \"exp6789\", \n",
    "            \"experiment_name\":\"My_run_listing_exp\", \n",
    "            \"stored_procedure\":\"run_experiment\",\n",
    "            \"algorithm_category\":\"Snowparkml\",\n",
    "            \"algorithm_type\":\"Classification\",\n",
    "            \"dataset_name\":\"EMPLOYEE_10L\",\n",
    "            \"target_column\":\"LEAVEORNOT\",\n",
    "            \"algorithms\":[{\"algorithm_name\": \"snowflake.ml.modeling.tree.DecisionTreeClassifier\"}]\n",
    "        }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d621e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-1 (classification Airline Delay dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"classification\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.xgboost.XGBClassifier\": null}}, \n",
    "\"dataset\": \"AIRLINE_DEP_DELAY_10K\", \n",
    "\"target_column\": \"DEP_DEL15\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df37eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-2 (Regression Alcohol dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"regression\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.ensemble.RandomForestRegressor\": null}}, \n",
    "\"dataset\": \"ALCOHOL_DATA_10L\", \n",
    "\"target_column\": \"QUALITY\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-3 (classification Employee dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"classification\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.linear_model.LogisticRegression\": null}}, \n",
    "\"dataset\": \"EMPLOYEE_10L\", \n",
    "\"target_column\": \"LEAVEORNOT\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-4 (Regression Diamonds dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"regression\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.xgboost.XGBRegressor\": null}}, \n",
    "\"dataset\": \"DIAMONDS\", \n",
    "\"target_column\": \"PRICE\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "\n",
    "session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101789b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection manager service url initialised to http://fdc-project-manager:80/project-manager\n",
      "If you need to update its value then update the variable CONNECTION_MANAGER_BASE_URL in os env.\n"
     ]
    }
   ],
   "source": [
    "from fosforio.manager import get_conn_details_from_ds_name\n",
    "from snowflake.snowpark.session import Session\n",
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e7bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage(session, stage_name=\"insight_exp\"):\n",
    "    try:\n",
    "        session.sql(f\"create or replace stage {stage_name}\").collect()\n",
    "        return f\"@{stage_name}\"\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake session\", ex)\n",
    "        raise ex\n",
    "\n",
    "        \n",
    "def get_session(dataset, project_id):\n",
    "    \"\"\"\n",
    "    Method creates snowflake session object.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_conn_details_from_ds_name(dataset, project_id)\n",
    "        region=conn[\"params\"][\"READER\"][\"region\"] if conn[\"params\"][\"READER\"][\"cloudPlatform\"] is None \\\n",
    "                    else conn[\"params\"][\"READER\"][\"region\"]+\".\"+conn[\"params\"][\"READER\"][\"cloudPlatform\"]\n",
    "        account = conn['params']['READER']['accountId'] if region is None \\\n",
    "                    else conn['params']['READER']['accountId']+\".\"+region\n",
    "        CONNECTION_PARAMETERS = {\n",
    "            \"account\": account,\n",
    "            \"user\":conn['params']['READER']['user'],\n",
    "            \"password\": conn['params']['READER']['password'],\n",
    "            \"role\": conn['params']['READER']['role'],\n",
    "            \"database\": conn['params']['READER']['database'],\n",
    "            \"warehouse\": conn['params']['READER']['wareHouse'],\n",
    "            \"schema\": conn['params']['READER']['schema']\n",
    "        }\n",
    "        return Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake session\", ex)\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec151ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(session: Session, exp_data: dict) -> list:\n",
    "    # Imports\n",
    "    from snowflake.ml.modeling.pipeline import Pipeline\n",
    "    from snowflake.ml.modeling.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "    from snowflake.ml.modeling.metrics import r2_score, accuracy_score, precision_score, roc_auc_score, \\\n",
    "        f1_score, recall_score, log_loss, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    from snowflake.snowpark.functions import col, is_null, regexp_replace, when, lit\n",
    "    from snowflake.snowpark.types import StringType\n",
    "    from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "    import importlib, sys, json, os, logging\n",
    "    from snowflake.ml.registry.registry import Registry\n",
    "    from fosforml import register_model\n",
    "    import os\n",
    "\n",
    "\n",
    "    # variable for holding logs\n",
    "    logs = []\n",
    "    \n",
    "    # function for accumulating logs\n",
    "    def log_message(level: str, message: str):\n",
    "        logs.append(f\"{level}: {message}\")\n",
    "    \n",
    "    \n",
    "    # Functions used in stored proc\n",
    "    def apply_data_cleansing(df):\n",
    "        \"\"\"\n",
    "        Method handles null values in snowpark dataframe.\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        :returns:\n",
    "        df_cleaned: dataframe after null handling\n",
    "        \"\"\"\n",
    "        # fillna - Unknown and 0\n",
    "        schema_fields = df.schema.fields\n",
    "        fill_values = {field.name: \"UNKNOWN\" if isinstance(field.datatype, StringType) else 0 for field in schema_fields}\n",
    "        df_cleaned = df.fillna(fill_values)\n",
    "        return df_cleaned\n",
    "\n",
    "\n",
    "    def get_feature_columns(df):\n",
    "        \"\"\"\n",
    "        Identifies the numerical and categorical features in dataset.\n",
    "        Identifies features for label encoding and one hot encoding\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        :returns:\n",
    "        categorical_features: list of non-numerical feature columns\n",
    "        numerical_features: list of numerical feature columns\n",
    "        le_column_features: list of feature label encoder columns\n",
    "        oh_column_features: list of feature one hot encoder columns\n",
    "        \"\"\"\n",
    "        schema_fields = df.schema.fields\n",
    "        features = df.columns\n",
    "        features.remove(exp_details.get(\"target_column\"))\n",
    "        df_schema = session.sql(f\"DESCRIBE TABLE {exp_details.get('dataset_name')}\").collect()\n",
    "        categorical_types = ['VARCHAR','CHAR','STRING','TEXT','BOOL']\n",
    "        categorical_features = []  \n",
    "        for row in df_schema:\n",
    "            for typ in categorical_types:\n",
    "                if typ in row['type']:\n",
    "                    categorical_features.append(row['name'])\n",
    "                    break\n",
    "        numerical_features = list(set(features) - set(categorical_features))\n",
    "        print(f\"numerical_features:  {numerical_features}\")\n",
    "        print(f\"categorical_features: {categorical_features}\")\n",
    "        log_message(\"INFO\",f\"numerical_features:  {numerical_features}\")\n",
    "        log_message(\"INFO\",f\"categorical_features: {categorical_features}\")\n",
    "        \n",
    "        \n",
    "        #identify columns for labelencoding and onehotencoding   \n",
    "        le_column_features = categorical_features\n",
    "        oh_column_features = []\n",
    "        if len(categorical_features) >= 1:\n",
    "            log_message(\"INFO\",f\"{categorical_features} columns are non numeric in feature dataset, encoding required.\")\n",
    "            for column in categorical_features:\n",
    "                if df.select(df[column]).distinct().count() < 10:\n",
    "                    oh_column_features.append(column)\n",
    "            print(f\"Columns identified to be encoded with label encoder: {le_column_features}\")\n",
    "            print(f\"Columns identified to be encoded with one hot encoder: {oh_column_features}\")\n",
    "            log_message(\"INFO\",f\"Columns identified to be encoded with label encoder: {le_column_features}\")\n",
    "            log_message(\"INFO\",f\"Columns identified to be encoded with one hot encoder: {oh_column_features}\")\n",
    "        return categorical_features, numerical_features, le_column_features, oh_column_features\n",
    "\n",
    "\n",
    "    def create_and_run_preprocessing(df, categorical_features, numerical_features, le_column_features, oh_column_features):\n",
    "        \"\"\"\n",
    "        Based on different features column input creates preprocessing steps and run it on input dataframe\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        categorical_features: list of non-numerical feature columns\n",
    "        numerical_features: list of numerical feature columns\n",
    "        le_column_features: list of feature label encoder columns\n",
    "        oh_column_features: list of feature one hot encoder columns\n",
    "        :returns:\n",
    "        df_train: preprocessed train dataset\n",
    "        df_test: preprocessed test dataset\n",
    "        \"\"\"\n",
    "        #pipeline steps \n",
    "        log_message(\"INFO\",\"Setting up preprocessing pipeline based on dataset\")\n",
    "        categorical_pp = {f'le_{column}':LabelEncoder(input_cols=column, output_cols=column) for column in le_column_features}\n",
    "        if len(oh_column_features)>0:\n",
    "            categorical_pp['oh_enc'] = OneHotEncoder(input_cols=oh_column_features, output_cols=oh_column_features, handle_unknown='ignore')\n",
    "        numerical_pp = {\n",
    "            'scaler': MinMaxScaler(input_cols=numerical_features, output_cols=numerical_features)\n",
    "        }\n",
    "        steps = [(key, categorical_pp[key]) for key in categorical_pp if categorical_pp[key]!=[]] + \\\n",
    "        [(key, numerical_pp[key]) for key in numerical_pp if numerical_features!=[]]\n",
    "        print(f\"Selected preprocesing steps: \\n{steps}\")\n",
    "        log_message(\"INFO\",f\"Selected preprocesing steps: \\n{steps}\")   \n",
    "            \n",
    "        # Run preprocessing pipeline steps\n",
    "        print(\"Running data preprocessing pipeline\")\n",
    "        log_message(\"INFO\",\"Running data preprocessing pipeline\") \n",
    "        df = Pipeline(steps=steps).fit(df).transform(df)\n",
    "        print(f\"Transformed dataset: \\n {df.show()}\")\n",
    "        log_message(\"INFO\",f\"Transformed dataset: \\n {df.show()}\")\n",
    "        df_train, df_test = df.random_split(weights=[0.8, 0.2], seed=0)\n",
    "        for col_name in df_train.schema.names:\n",
    "            new_col = col_name.replace('.','_')\n",
    "            df_train = df_train.withColumnRenamed(col_name, new_col)\n",
    "        for col_name in df_test.schema.names:\n",
    "            new_col = col_name.replace('.','_')\n",
    "            df_test = df_test.withColumnRenamed(col_name, new_col)\n",
    "        return df_train, df_test\n",
    "\n",
    "\n",
    "    def run_estimator(df_train, df_test, input_cols):\n",
    "        \"\"\"\n",
    "        trains on df_train and creates model object for given algorithm/estimator.\n",
    "        runs prediction function of model object on test dataset\n",
    "        :param:\n",
    "        df_train: input training dataframe\n",
    "        df_test: input test dataframe\n",
    "        input_cols: list of input feature names\n",
    "        :returns:\n",
    "        model: trained model object\n",
    "        df_pred: output predict dataframe\n",
    "        \"\"\"\n",
    "        for algorithm in exp_details.get(\"algorithms\"):\n",
    "            algorithm = algorithm[\"algorithm_name\"].rsplit('.', 1)\n",
    "            module = importlib.import_module(algorithm[0])\n",
    "            print(f\"----Running Algorithm {algorithm[1]}----\")\n",
    "            log_message(\"INFO\",f\"----Running Algorithm {algorithm[1]}----\")\n",
    "            attr = getattr(module, algorithm[1])\n",
    "            pipe = Pipeline(steps=[(\"algorithm\", attr(input_cols=input_cols\n",
    "                                                  , label_cols=[exp_details.get(\"target_column\")]\n",
    "                                                  , output_cols=['PREDICTIONS']))]\n",
    "                   )\n",
    "    \n",
    "            # Fit the pipeline\n",
    "            print(f\"Running model pipeline {algorithm[1]}\")\n",
    "            log_message(\"INFO\",f\"Running model pipeline {algorithm[1]}\")\n",
    "            model = pipe.fit(df_train)\n",
    "            print(dir(model))\n",
    "            # Test the model\n",
    "            print(\"Running prediction on model with test dataset\")\n",
    "            log_message(\"INFO\",\"Running prediction on model with test dataset\")\n",
    "            df_pred = model.predict(df_test)\n",
    "            return model, df_pred, algorithm[1]\n",
    "\n",
    "    \n",
    "    def try_or(fn):\n",
    "        try:\n",
    "            out = fn()\n",
    "            return out\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "        \n",
    "    def eval_metrics(df_pred):\n",
    "        log_message(\"INFO\",\"Generating Metrices\")\n",
    "        if exp_details.get(\"algorithm_type\") == 'Classification':\n",
    "            return {\n",
    "            'accuracy_score': try_or(lambda: accuracy_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'f1_score': try_or(lambda: f1_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'recall_score': try_or(lambda: recall_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'precision_score': try_or(lambda: precision_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'roc_auc': try_or(lambda: roc_auc_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_score_col_names='PREDICTIONS')),\n",
    "            'log_loss': try_or(lambda: log_loss(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS'))\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "            'mean_absolute_percentage_error': try_or(lambda: mean_absolute_percentage_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'r2_score': try_or(lambda: r2_score(df=df_pred, y_true_col_name=exp_details.get(\"target_column\"), y_pred_col_name='PREDICTIONS')),\n",
    "            'mean_absolute_error': try_or(lambda: mean_absolute_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'mean_squared_error': try_or(lambda: mean_squared_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS'))\n",
    "            }\n",
    "\n",
    "\n",
    "#     def register_model(model, metrics_info, estimator):\n",
    "#         log_message(\"INFO\",\"Started: Registering model on snowflake\")\n",
    "#         reg = Registry(session=session)\n",
    "        \n",
    "#         clean = lambda x : x.replace(\"-\",\"_\")\n",
    "#         project_id = clean(exp_details.get(\"project_id\"))\n",
    "#         exp_name = clean(exp_details.get(\"experiment_name\"))\n",
    "#         run_id = clean(exp_details.get(\"monitor_run_id\"))\n",
    "#         model_name = \"Experiment_\"+project_id+\"_FDC_\"+exp_name\n",
    "#         mv = reg.log_model(model=model,\n",
    "#                            model_name=model_name,\n",
    "#                            comment=exp_details.get(\"description\"),\n",
    "#                            version_name='V_'+exp_details.get(\"monitor_run_id\"),\n",
    "#                            python_version=\"3.9.19\",\n",
    "#                            conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "#                            metrics={\"model_metrics\": metrics_info,\n",
    "#                                     \"dataset_details\": {\n",
    "#                                         \"dataset_name\": exp_details.get(\"dataset_name\"),\n",
    "#                                         \"target_column\": exp_details.get(\"target_column\"),\n",
    "#                                     },\n",
    "#                                     \"hyper_parameters\": {'criterion': 'entropy', 'max_depth': None, 'max_features': 8, 'min_samples_leaf': 7},\n",
    "#                                      \"algorithm_type\": exp_details.get(\"algorithm_type\"),\n",
    "#                                      \"algorithm\": estimator,                                   \n",
    "#                                      \"experiment_name\": exp_details.get(\"experiment_name\"),\n",
    "#                                      \"source\": \"Experiment\",\n",
    "#                                      \"status\": \"Trained\",\n",
    "#                                      \"run_id\": exp_details.get(\"monitor_run_id\"),\n",
    "#                                      \"experiment_id\": exp_details.get(\"experiment_id\")})\n",
    "#         log_message(\"INFO\",\"Registeration of model completed!!!\")\n",
    "#         return model_name\n",
    "            \n",
    "            \n",
    "    def create_tags(session, exp_details):\n",
    "        for key in exp_details.keys():\n",
    "            tag = session.sql(f\"CREATE TAG IF NOT EXISTS {key}\")\n",
    "            tag.show()\n",
    "        if exp_details.get('algorithm_type')=='Classification':\n",
    "            metric_names=[\"accuracy_score\",\"precision_score\",\"recall_score\",\"f1_score\",\"roc_auc\",\"log_loss\"]\n",
    "        else:\n",
    "            metric_names=[\"r2_score\",\"mean_absolute_error\",\"mean_squared_error\",\"mean_absolute_percentage_error\"]\n",
    "        for name in metric_names:\n",
    "            tag = session.sql(f\"CREATE TAG IF NOT EXISTS {name}\")\n",
    "            tag.show()\n",
    "\n",
    "        \n",
    "    def set_tags(session, m_name, exp_details, metric_info):\n",
    "        for key, value in exp_details.items():\n",
    "            value = str(value).replace(\"'\",\"\\\"\")\n",
    "            tag = session.sql(f\"ALTER MODEL IF EXISTS {m_name} SET TAG {key}='{value}'\")\n",
    "            tag.show()\n",
    "        for key, value in metric_info.items():\n",
    "            value = str(value).replace(\"'\",\"\\\"\")\n",
    "            tag = session.sql(f\"ALTER MODEL IF EXISTS {m_name} SET TAG {key}='{value}'\")\n",
    "            tag.show()\n",
    "            \n",
    "    def register_model_using_fosforml(model, df_pred):\n",
    "        print(\"Registering model\")\n",
    "        log_message(\"INFO\", f\"Registering model using fosforml\")\n",
    "        response = register_model(\n",
    "            model_obj=model,\n",
    "            session=session,\n",
    "            name=exp_data.get(\"experiment_name\", \"default\"),\n",
    "            snowflake_df=df_pred,\n",
    "            dataset_name=exp_data.get(\"dataset_name\"),\n",
    "            dataset_source=\"SnowflakeDataset\",\n",
    "            description=exp_data.get(\"description\",\"This is an experiment generated model\"),\n",
    "            flavour=\"snowflake\",\n",
    "            model_type=exp_data.get(\"algorithm_type\").lower(),\n",
    "            conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "            prediction_column=\"PREDICTIONS\",\n",
    "            source=\"Experiment\"\n",
    "        )\n",
    "        model_name = response.split(\" \")[1]\n",
    "        print(\"Model Name: \", model_name)\n",
    "        return response\n",
    "\n",
    "    try:\n",
    "        # loading experiment details\n",
    "        exp_details=exp_data\n",
    "        os.environ[\"RUN_ID\"] = os.environ['run_id'] = exp_data.get(\"monitor_run_id\")\n",
    "        os.environ[\"algorithm_type\"] = exp_data.get(\"algorithm_type\")\n",
    "        os.environ[\"userId\"] = exp_data.get(\"userId\")\n",
    "        os.environ[\"experiment_name\"] = exp_data.get(\"experiment_name\", \"\")\n",
    "        os.environ[\"experiment_id\"] = exp_data.get(\"experiment_id\")\n",
    "        os.environ[\"PROJECT_ID\"] = exp_data.get(\"project_id\")\n",
    "\n",
    "        # creating user tags if not exist\n",
    "        create_tags(session, exp_details)\n",
    "\n",
    "        # Reading dataset\n",
    "        log_message(\"INFO\",\"Reading dataset features\")\n",
    "        data = session.table(exp_details.get(\"dataset_name\"))\n",
    "\n",
    "        # fillna\n",
    "        data = apply_data_cleansing(data)\n",
    "\n",
    "        # Identify feature columns\n",
    "        categorical_features, numerical_features, le_column_features, oh_column_features = get_feature_columns(data)\n",
    "\n",
    "        # Based on feature, do preprocessing\n",
    "        data_train, data_test = create_and_run_preprocessing(data, categorical_features, numerical_features, le_column_features, oh_column_features)\n",
    "        \n",
    "        # Save Test and Train Dataset\n",
    "#         data_train.write.mode(\"overwrite\").save_as_table(\"EMPLOYEE_TRAIN_CLEANED\")\n",
    "#         data_train.write.mode(\"overwrite\").save_as_table(\"EMPLOYEE_TEST_CLEANED\")\n",
    "        \n",
    "        # Run model training and prediction\n",
    "        input_cols = data_train.columns\n",
    "        input_cols.remove(exp_details.get(\"target_column\"))\n",
    "        model, data_pred, estimator = run_estimator(data_train, data_test, input_cols)\n",
    "        \n",
    "#         print(str(type(model)))\n",
    "\n",
    "        # Register model on snowflake registry\n",
    "        response = register_model_using_fosforml(model, data_pred)\n",
    "        if response.split(\" \")[1].startswith(\"'EXPERIMENT\"):\n",
    "            print(f\"Model registered on snowflake registry by name: {response.split(' ')[1]}\")\n",
    "            log_message(\"INFO\", f\"Model registered on snowflake registry by name: {response}\")\n",
    "        else:\n",
    "            print(f\"Model registration failed: {model_name}\")\n",
    "            log_message(\"INFO\", f\"Model registration failed: {model_name}\")\n",
    "            raise Exception(f\"Model registration failed, {model_name}\")\n",
    "\n",
    "        return [{\"Execution Logs:\": \"\\n\".join(logs),\n",
    "                 \"experiment_name\":exp_data.get(\"experiment_name\"),\n",
    "                 \"algorithm_type\":exp_data.get(\"algorithm_type\"),\n",
    "                 \"algorithm\": estimator,\n",
    "                 \"dataset_name\": exp_data.get(\"dataset_name\"),\n",
    "                 \"target_column\": exp_data.get(\"target_column\"),\n",
    "                 \"run_status\": \"SUCCESS\",\n",
    "                 \"registry_model_name\": model_name}]\n",
    "    except SnowparkSQLException as snowex:\n",
    "        log_message(\"ERROR\", f\"{str(snowex).split('?')}\")\n",
    "        raise Exception(\"Execution Logs: \"+\"\\n\".join(logs))\n",
    "    except Exception as ex:\n",
    "        # log_message(\"ERROR\", f\"{repr(ex)}\")\n",
    "        log_message(\"ERROR\", f\"{repr(ex)}\")\n",
    "        raise Exception(\"Execution Logs: \"+\"\\n\".join(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfdd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initilization\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print(\"Creating Snowflake Session object...\")\n",
    "# exp_details=json.loads(_exp_data)\n",
    "session = get_session(_exp_data.get(\"dataset_name\"),\"fd0cf79c-8118-43a7-8f0b-059e8f78227a\")\n",
    "stage = create_stage(session)\n",
    "print(\"Session has been created !\")\n",
    "print(\"Adding fosforml to stage\")\n",
    "session.sql(f\"PUT file://fosforml.zip {stage}\").collect()\n",
    "print(\"Creating stored procedure...\")\n",
    "session.custom_package_usage_config['enabled'] = True\n",
    "session.custom_package_usage_config['force_push'] = True\n",
    "# session.custom_package_usage_config[\"cache_path\"] = stage+\"/run_experiment\"\n",
    "session.sproc.register(func=run_experiment,\n",
    "                       name=\"run_experiment\",\n",
    "                       imports=[f\"{stage}/fosforml.zip\"],\n",
    "                       packages=[\"snowflake-snowpark-python==1.15.0\", \n",
    "                                 \"snowflake-ml-python==1.5.0\",\n",
    "                                 \"scikit-learn==1.3.2\"],\n",
    "                       is_permanent=True,\n",
    "                       stage_location=stage,\n",
    "                       replace=True)\n",
    "print(\"Stored procedure has been created successfully!\")\n",
    "\n",
    "# tagging session\n",
    "print(\"Setting tag to session object: tag= \", \"110003\")\n",
    "session.query_tag=\"110003\"\n",
    "\n",
    "print(\"Executing Procedure\")\n",
    "procedure_response = session.call(\"run_experiment\", _exp_data)\n",
    "# procedure_response = run_experiment(session, _exp_data)\n",
    "print(\"Stored Procedure Executed Successfully !\")\n",
    "print(procedure_response)\n",
    "\n",
    "#Log in mlflow\n",
    "print(\"Logging in mlflow completed !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, df_pred = run_experiment(session, _exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc68c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_column = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = model.output_cols[0] if all([hasattr(model,\"output_cols\"),len(model.output_cols)>0]) else prediction_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71708e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all([hasattr(model,\"output_cols\"),len(model.output_cols)>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce66e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(type(model)).find(\"snowflake\") > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.output_cols)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843971b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(model,'n_classes_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b47ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef50d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model = model.to_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b297ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121260cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml import register_model\n",
    "response = register_model(\n",
    "            model_obj=model,\n",
    "            session=session,\n",
    "            name=\"First_FosforML\",\n",
    "            snowflake_df=df_pred,\n",
    "            dataset_name=_exp_data.get(\"dataset_name\"),\n",
    "            dataset_source=\"SnowflakeDataset\",\n",
    "            description=\"This is a test model\",\n",
    "            flavour=\"snowflake\",\n",
    "            model_type=\"classification\",\n",
    "            conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "            prediction_column=\"PREDICTIONS\",\n",
    "            source=\"Experiment\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc91bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml.model_manager import snowflakesession\n",
    "obj = snowflakesession()\n",
    "obj.connection_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c60f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg  =  Registry(\n",
    "    session= session,\n",
    "    database_name=\"FIRST_DB\",\n",
    "    schema_name=\"PUBLIC\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce565b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"SELECT TO_CHAR(CURRENT_TIMESTAMP,'YYYY-MM-DD HH24:MI:SS')\").to_pandas().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c770ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "session = get_session('EMPLOYEE',\"fd0cf79c-8118-43a7-8f0b-059e8f78227a\") #\"f4967384-6aec-4483-8b4b-ede17e74842b\" fd0cf79c-8118-43a7-8f0b-059e8f78227a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = session.table('EMPLOYEE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6986b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b7813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72eb352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = session.table('STUDENT_PERFORMANCE_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ddecd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a85f719",
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "252006: Cursor is closed in execute.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_as_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSTUDENT_PERFORMANCE_DATA_WITH_QUOTES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/telemetry.py:173\u001b[0m, in \u001b[0;36mdfw_collect_api_telemetry.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_dataframe\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mquery_history() \u001b[38;5;28;01mas\u001b[39;00m query_history:\n\u001b[0;32m--> 173\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     plan \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_dataframe\u001b[38;5;241m.\u001b[39m_select_statement \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_dataframe\u001b[38;5;241m.\u001b[39m_plan\n\u001b[1;32m    175\u001b[0m     api_calls \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;241m*\u001b[39mplan\u001b[38;5;241m.\u001b[39mapi_calls,\n\u001b[1;32m    177\u001b[0m         {TelemetryField\u001b[38;5;241m.\u001b[39mNAME\u001b[38;5;241m.\u001b[39mvalue: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrameWriter.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    178\u001b[0m     ]\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/dataframe_writer.py:231\u001b[0m, in \u001b[0;36mDataFrameWriter.save_as_table\u001b[0;34m(self, table_name, mode, column_order, create_temp_table, table_type, clustering_keys, statement_params, block, comment)\u001b[0m\n\u001b[1;32m    221\u001b[0m create_table_logic_plan \u001b[38;5;241m=\u001b[39m SnowflakeCreateTable(\n\u001b[1;32m    222\u001b[0m     table_name,\n\u001b[1;32m    223\u001b[0m     column_names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     comment,\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataframe\u001b[38;5;241m.\u001b[39m_session\n\u001b[0;32m--> 231\u001b[0m snowflake_plan \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_table_logic_plan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m result \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m    233\u001b[0m     snowflake_plan,\n\u001b[1;32m    234\u001b[0m     _statement_params\u001b[38;5;241m=\u001b[39mstatement_params \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataframe\u001b[38;5;241m.\u001b[39m_statement_params,\n\u001b[1;32m    235\u001b[0m     block\u001b[38;5;241m=\u001b[39mblock,\n\u001b[1;32m    236\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m_AsyncResultType\u001b[38;5;241m.\u001b[39mNO_RESULT,\n\u001b[1;32m    237\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/analyzer/analyzer.py:724\u001b[0m, in \u001b[0;36mAnalyzer.resolve\u001b[0;34m(self, logical_plan)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubquery_plans \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerated_alias_maps \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 724\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogical_plan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m result\u001b[38;5;241m.\u001b[39madd_aliases(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerated_alias_maps)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubquery_plans:\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/analyzer/analyzer.py:767\u001b[0m, in \u001b[0;36mAnalyzer.do_resolve\u001b[0;34m(self, logical_plan)\u001b[0m\n\u001b[1;32m    761\u001b[0m             use_maps\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    762\u001b[0m                 {p: q \u001b[38;5;28;01mfor\u001b[39;00m p, q \u001b[38;5;129;01min\u001b[39;00m v\u001b[38;5;241m.\u001b[39mexpr_to_alias\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m counts[p] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m}\n\u001b[1;32m    763\u001b[0m             )\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malias_maps_to_use \u001b[38;5;241m=\u001b[39m use_maps\n\u001b[0;32m--> 767\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_resolve_with_resolved_children\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogical_plan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolved_children\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_aliased_col_name_to_real_col_name\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m res\u001b[38;5;241m.\u001b[39mdf_aliased_col_name_to_real_col_name\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    771\u001b[0m     df_aliased_col_name_to_real_col_name\n\u001b[1;32m    772\u001b[0m )\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/analyzer/analyzer.py:945\u001b[0m, in \u001b[0;36mAnalyzer.do_resolve_with_resolved_children\u001b[0;34m(self, logical_plan, resolved_children, df_aliased_col_name_to_real_col_name)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplan_builder\u001b[38;5;241m.\u001b[39mtable(logical_plan\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logical_plan, SnowflakeCreateTable):\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_as_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogical_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogical_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogical_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogical_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_aliased_col_name_to_real_col_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlogical_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclustering_exprs\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogical_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_children\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogical_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logical_plan, Limit):\n\u001b[1;32m    959\u001b[0m     on_top_of_order_by \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    960\u001b[0m         logical_plan\u001b[38;5;241m.\u001b[39mchild, SnowflakePlan\n\u001b[1;32m    961\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logical_plan\u001b[38;5;241m.\u001b[39mchild\u001b[38;5;241m.\u001b[39msource_plan, Sort)\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:710\u001b[0m, in \u001b[0;36mSnowflakePlanBuilder.save_as_table\u001b[0;34m(self, table_name, column_names, mode, table_type, clustering_keys, comment, child)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# here get the column definition from the child attributes. In certain cases we have\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# the attributes set to ($1, VariantType()) which cannot be used as valid column name\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;66;03m# in save as table. So we rename ${number} with COL{number}.\u001b[39;00m\n\u001b[1;32m    708\u001b[0m hidden_column_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m column_definition_with_hidden_columns \u001b[38;5;241m=\u001b[39m attribute_to_schema_string(\n\u001b[0;32m--> 710\u001b[0m     \u001b[43mchild\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattributes\u001b[49m\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    712\u001b[0m column_definition \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    713\u001b[0m     hidden_column_pattern,\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m match: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOL\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatch\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    715\u001b[0m     column_definition_with_hidden_columns,\n\u001b[1;32m    716\u001b[0m )\n\u001b[1;32m    718\u001b[0m child \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mreplace_repeated_subquery_with_cte()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/functools.py:993\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    991\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 993\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    995\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:319\u001b[0m, in \u001b[0;36mSnowflakePlan.attributes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattributes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Attribute]:\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# No simplifier case relies on this schema_query change to update SHOW TABLES to a nested sql friendly query.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema_query \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39msql_simplifier_enabled:\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/analyzer/schema_utils.py:91\u001b[0m, in \u001b[0;36manalyze_attributes\u001b[0;34m(sql, session)\u001b[0m\n\u001b[1;32m     88\u001b[0m     session\u001b[38;5;241m.\u001b[39m_run_query(sql)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_result_meta_to_attribute(session\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39m_cursor\u001b[38;5;241m.\u001b[39mdescription)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_result_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/session.py:2109\u001b[0m, in \u001b[0;36mSession._get_result_attributes\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_result_attributes\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Attribute]:\n\u001b[0;32m-> 2109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:122\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mProgrammingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    124\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/server_connection.py:233\u001b[0m, in \u001b[0;36mServerConnection.get_result_attributes\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;129m@SnowflakePlan\u001b[39m\u001b[38;5;241m.\u001b[39mDecorator\u001b[38;5;241m.\u001b[39mwrap_exception\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_result_attributes\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Attribute]:\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_result_meta_to_attribute(\u001b[43mrun_new_describe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/analyzer/schema_utils.py:144\u001b[0m, in \u001b[0;36mrun_new_describe\u001b[0;34m(cursor, query)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# ResultMetadataV2 may not currently be a type, depending on the connector\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# version, so the argument types are pyright ignored\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cursor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_describe_internal\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# Pyright does not perform narrowing here\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_describe_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescribe(query)\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/cursor.py:1120\u001b[0m, in \u001b[0;36mSnowflakeCursor._describe_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Obtain the schema of the result without executing the query.\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03mThis function takes the same arguments as execute, please refer to that function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03m    The schema of the result, in the new result metadata format.\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_describe_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_internal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_description\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/cursor.py:912\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements)\u001b[0m\n\u001b[1;32m    910\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecuting SQL/command\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed():\n\u001b[0;32m--> 912\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mInterfaceError\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmsg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCursor is closed in execute.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merrno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mER_CURSOR_IS_CLOSED\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _do_reset:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/errors.py:290\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    269\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    298\u001b[0m             error_class,\n\u001b[1;32m    299\u001b[0m             error_value,\n\u001b[1;32m    300\u001b[0m         )\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/errors.py:345\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 345\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/errors.py:221\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    219\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[1;32m    222\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    223\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[1;32m    224\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    225\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    226\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    227\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[1;32m    229\u001b[0m     ),\n\u001b[1;32m    230\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    231\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[1;32m    232\u001b[0m )\n",
      "\u001b[0;31mInterfaceError\u001b[0m: 252006: Cursor is closed in execute."
     ]
    }
   ],
   "source": [
    "data.write.mode(\"overwrite\").save_as_table(\"STUDENT_PERFORMANCE_DATA_WITH_QUOTES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b7b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'GradeClass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32750bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    if f'\"{target_column}\"' in feature or f\"'{target_column}'\" in feature:\n",
    "        target_column = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b318d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a383f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = session.sql(f\"DESCRIBE TABLE STUDENT_PERFORMANCE_DATA\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_types = ['VARCHAR','CHAR','STRING','TEXT','BOOL']\n",
    "categorical_features = []  \n",
    "for row in df_schema:\n",
    "    for typ in categorical_types:\n",
    "        if typ in row['type']:\n",
    "            categorical_features.append(row['name'])\n",
    "            break\n",
    "numerical_features = list(set(features) - set(categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a64c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.remove(target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e2aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    if f'\"{target_column}\"' in feature or f\"'{target_column}'\" in feature:\n",
    "        target_column = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = session.table('HR_BINARY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
