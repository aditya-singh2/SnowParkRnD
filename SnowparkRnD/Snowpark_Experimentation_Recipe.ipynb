{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9b83dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fosforml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/c9/1073eb02a766d41efc24798974af5b1ec2155688554f6e6c2039a4a16578/fosforml-1.0.9-py3-none-any.whl (43kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 568kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cloudpickle==2.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/15/80/44286939ca215e88fa827b2aeb6fa3fd2b4a7af322485c7170d6f9fd96e0/cloudpickle-2.2.1-py3-none-any.whl\n",
      "Collecting snowflake-ml-python==1.5.0; python_version <= \"3.9\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/72/c0fa5a9bc811a59a5a1c7113ff89676ed1629d7d6463db8c1a8c97a8b5f6/snowflake_ml_python-1.5.0-py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 2.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==1.3.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/89/dce01a35d354159dcc901e3c7e7eb3fe98de5cb3639c6cd39518d8830caa/scikit_learn-1.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9MB)\n",
      "\u001b[K     |████████████████████████████████| 10.9MB 47.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]<2024,>=2022.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/25/fab23259a52ece5670dcb8452e1af34b89e6135ecc17cd4b54b4b479eac6/fsspec-2023.12.2-py3-none-any.whl (168kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 55.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions<5,>=4.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl\n",
      "Collecting retrying<2,>=1.3.3\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/04/9e36f28be4c0532c0e9207ff9dc01fb13a2b0eb036476a213b0000837d0e/retrying-1.3.4-py3-none-any.whl\n",
      "Collecting sqlparse<1,>=0.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/5d/a0fdd88fd486b39ae1fd1a75ff75b4e29a0df96c0304d462fd407b82efe0/sqlparse-0.5.0-py3-none-any.whl (43kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 9.6MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6,>=3.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/2b/a64c2d25a37aeb921fddb929111413049fc5f8b9a4c1aefaffaafe768d54/cachetools-5.3.3-py3-none-any.whl\n",
      "Collecting snowflake-snowpark-python!=1.12.0,<2,>=1.11.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/ab/62e3cd641d49a079fa34bc911df38937f867b76947d85465f36c44d8da50/snowflake_snowpark_python-1.19.0-py3-none-any.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 44.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs<2024,>=2022.11\n",
      "  Downloading https://files.pythonhosted.org/packages/5b/d6/b8a748b7d3fc7b0fd2ede1cf26a80281d65cc24d5d56b66c3a4c87e256e2/s3fs-2023.12.2-py3-none-any.whl\n",
      "Collecting importlib-resources<7,>=6.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/75/06/4df55e1b7b112d183f65db9503bff189e97179b256e1ea450a3c365241e0/importlib_resources-6.4.0-py3-none-any.whl\n",
      "Collecting pyarrow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/3e/9cfa78ad9744c77e4f3c183d919de3649505e50663d3715151a094c27769/pyarrow-16.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.0MB)\n",
      "\u001b[K     |████████████████████████████████| 41.0MB 38.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xgboost<2,>=1.7.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/3a/c9c5d4d5c49b132ef15ac7b5ccf56ef1c82efe36cd19414771762e97c00e/xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3MB)\n",
      "\u001b[K     |████████████████████████████████| 200.3MB 71kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting snowflake-connector-python[pandas]<4,>=3.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/ab/a476fc0fddf4d3133d4a91c9b3ac0357b33ea49888ef0b55724fa79786d1/snowflake_connector_python-3.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5MB 34.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catboost<1.3,>=1.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/05/b1e05aeccbe310be4f44d6c9065b13600c4d72bcfbe2676bfad7f563a680/catboost-1.2.5-cp39-cp39-manylinux2014_x86_64.whl (98.2MB)\n",
      "\u001b[K     |████████████████████████████████| 98.2MB 147kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting packaging<24,>=20.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/1a/610693ac4ee14fcdf2d9bf3c493370e4f2ef7ae2e19217d7a237ff42367d/packaging-23.2-py3-none-any.whl (53kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 11.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy<2,>=1.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f5/d0ad1a96f80962ba65e2ce1de6a1e59edecd1f0a7b55990ed208848012e0/scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6MB)\n",
      "\u001b[K     |████████████████████████████████| 38.6MB 32.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas<3,>=1.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/30/f6f1f1ac36250f50c421b1b6af08c35e5a8b5a84385ef928625336b93e6f/pandas-2.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1MB 49.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting anyio<4,>=3.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl (80kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 13.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<2,>=1.23\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/30/c2a907b9443cf42b90c17ad10c1e8fa801975f01cb9764f3f8eb8aea638b/numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2MB)\n",
      "\u001b[K     |████████████████████████████████| 18.3MB 51.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytimeparse<2,>=1.1.8\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/b4/afd75551a3b910abd1d922dbd45e49e5deeb4d47dc50209ce489ba9844dd/pytimeparse-1.1.8-py2.py3-none-any.whl\n",
      "Collecting absl-py<2,>=0.15\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/87/de5c32fa1b1c6c3305d576e299801d8655c175ca9557019906247b994331/absl_py-1.4.0-py3-none-any.whl (126kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 30.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml<7,>=6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/39/472f2554a0f1e825bd7c5afc11c817cd7a2f3657460f7159f691fbb37c51/PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738kB)\n",
      "\u001b[K     |████████████████████████████████| 747kB 32.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl\n",
      "Collecting joblib>=1.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 44.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/4f/3c99f1cdab4fb55e12a914c80828f0958f04d79ef6b6ce1e05d07c30c46b/aiohttp-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 51.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests; extra == \"http\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "Collecting wheel\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/cd/d7460c9a869b16c3dd4e1e403cce337df165368c71d6af229a74699622ce/wheel-0.43.0-py3-none-any.whl (65kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 12.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools>=40.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/15/88e46eb9387e905704b69849618e699dc2f54407d8953cc4ec4b8b46528d/setuptools-70.3.0-py3-none-any.whl (931kB)\n",
      "\u001b[K     |████████████████████████████████| 931kB 38.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiobotocore<3.0.0,>=2.5.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/07/42f884c1600169e4267575cdd261c75dea31782d8fd877bbea358d559416/aiobotocore-2.13.1-py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 12.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zipp>=3.1.0; python_version < \"3.10\"\n",
      "  Downloading https://files.pythonhosted.org/packages/20/38/f5c473fe9b90c8debdd29ea68d5add0289f1936d6f923b6b9cc0b931194c/zipp-3.19.2-py3-none-any.whl\n",
      "Collecting pyjwt<3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/4f/e04a8067c7c96c364cef7ef73906504e2f40d690811c021e1a1901473a19/PyJWT-2.8.0-py3-none-any.whl\n",
      "Collecting urllib3<2.0.0,>=1.21.1; python_version < \"3.10\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/6a/99eaaeae8becaa17a29aeb334a18e5d582d873b6f084c11f02581b8d7f7f/urllib3-1.26.19-py2.py3-none-any.whl (143kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 55.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tomlkit\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/7c/b753bf603852cab0a660da6e81f4ea5d2ca0f0b2b4870766d7aa9bceb7a2/tomlkit-0.13.0-py3-none-any.whl\n",
      "Collecting cryptography<43.0.0,>=3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/1c/9f6d13cc8041c05eebff1154e4e71bedd1db8e174fff999054435994187a/cryptography-42.0.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9MB 20.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyOpenSSL<25.0.0,>=16.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/a7/2104f674a5a6845b04c8ff01659becc6b8978ca410b82b94287e0b1e018b/pyOpenSSL-24.1.0-py3-none-any.whl (56kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 10.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cffi<2.0.0,>=1.9\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/ac/e9e77bc385729035143e54cc8c4785bd480eaca9df17565963556b0b7a93/cffi-1.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 50.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/3d/a121f284241f08268b21359bd425f7d4825cffc5ac5cd0e1b3d82ffd2b10/pytz-2024.1-py2.py3-none-any.whl (505kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 51.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/7f/09065fd9e27da0eda08b4d6897f1c13535066174cc023af248fc2a8d5e5a/asn1crypto-1.5.1-py2.py3-none-any.whl (105kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 52.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/69/5d8751b4b670d623aa7a47bef061d69c279e9f922f6705147983aa76c3ce/charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 50.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3e/741d8c82801c347547f8a2a06aa57dbb1992be9e948df2ea0eda2c8b79e8/idna-3.7-py3-none-any.whl (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/d5/c84e1a17bf61d4df64ca866a1c9a913874b4e9bdc131ec689a0ad013fb36/certifi-2024.7.4-py3-none-any.whl (162kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 37.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sortedcontainers>=2.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl\n",
      "Collecting platformdirs<5.0.0,>=2.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/68/13/2aa1f0e1364feb2c9ef45302f387ac0bd81484e9c9a4c5688a322fbdfd08/platformdirs-4.2.2-py3-none-any.whl\n",
      "Collecting filelock<4,>=3.5\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/f0/48285f0262fe47103a4a45972ed2f9b93e4c80b8fd609fa98da78b2a5706/filelock-3.15.4-py3-none-any.whl\n",
      "Collecting plotly\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/f8/b65cdd2be32e442c4efe7b672f73c90b05eab5a7f3f4115efe181d432c60/plotly-5.22.0-py3-none-any.whl (16.4MB)\n",
      "\u001b[K     |████████████████████████████████| 16.4MB 25.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphviz\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/be/d59db2d1d52697c6adc9eacaf50e8965b6345cc143f671e1ed068818d5cf/graphviz-0.20.3-py3-none-any.whl (47kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 9.6MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/67/e75134cb83d2e533e46d72e2033a413772efdc18291beb981f5d574a829f/matplotlib-3.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3MB)\n",
      "\u001b[K     |████████████████████████████████| 8.3MB 44.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 51.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl (345kB)\n",
      "\u001b[K     |████████████████████████████████| 348kB 34.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sniffio>=1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl\n",
      "Collecting exceptiongroup; python_version < \"3.11\"\n",
      "  Downloading https://files.pythonhosted.org/packages/02/cc/b7e31358aac6ed1ef2bb790a9746ac2c69bcb3c8588b41616914eb106eaf/exceptiongroup-1.2.2-py3-none-any.whl\n",
      "Collecting async-timeout<5.0,>=4.0; python_version < \"3.11\"\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl\n",
      "Collecting attrs>=17.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/44/827b2a91a5816512fcaf3cc4ebc465ccd5d598c45cefa6703fcf4a79018f/attrs-23.2.0-py3-none-any.whl (60kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 316kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/b0/6f1ebdabfb604e39a0f84428986b89ab55f246b64cddaa495f2c953e1f6b/frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 47.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ea/d7e961ea9b1b818a43b155ee512117be6ab9ab67c1e94967b2e64126e8e4/yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 48.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/a9/1f8d42c8103bcb1da6bb719f1bc018594b5acc8eae56b3fec4720ebee225/multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 44.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl\n",
      "Collecting wrapt<2.0.0,>=1.10.10\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/e7/459a8a4f40f2fa65eb73cb3f339e6d152957932516d18d0e996c7ae2d7ae/wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 12.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.34.132,>=1.34.70\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/01785fad12a9b1dbeffebd97cd226ea5923114057c64a610dd4eb8a28c7b/botocore-1.34.131-py3-none-any.whl (12.3MB)\n",
      "\u001b[K     |████████████████████████████████| 12.3MB 39.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aioitertools<1.0.0,>=0.5.1\n",
      "  Downloading https://files.pythonhosted.org/packages/45/66/d1a9fd8e6ff88f2157cb145dd054defb0fd7fe2507fe5a01347e7c690eab/aioitertools-0.11.0-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycparser\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/a3/a812df4e2dd5696d1f351d58b8fe16a405b234ad2886a0dab9183fb78109/pycparser-2.22-py3-none-any.whl (117kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 26.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tenacity>=6.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/3f/8ba87d9e287b9d385a02a7114ddcef61b26f86411e121c9003eb509a1773/tenacity-8.5.0-py3-none-any.whl\n",
      "Collecting pillow>=8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/3f/c02268d0c6fb6b3958bdda673c17b315c821d97df29ae6969f20fb49388a/pillow-10.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4MB 47.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/a8/841594f11d0b88d8aeb26991bc4dac38baa909dc58d0c4262a4f7893bcbf/kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 39.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/ea/6d76df31432a0e6fdf81681a895f009a4bb47b3c39036db3e1b528191d52/pyparsing-3.1.2-py3-none-any.whl (103kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 28.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/a2/2f12e3a6e45935ff694654b710961b03310b0e1ec997ee9f416d3c873f87/contourpy-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 32.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl\n",
      "Collecting fonttools>=4.22.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/30/ad4483dfc5a1999f26b7bc5edc311576f433a3e00dd8aea01f2099c3a29f/fonttools-4.53.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6MB 46.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement cloudpickle==1.6.0, but you'll have cloudpickle 2.2.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement PyYAML==6.0, but you'll have pyyaml 6.0.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: refractml 1.0.3 has requirement urllib3==1.26.15, but you'll have urllib3 1.26.19 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: refractio 2.1.5.2 has requirement pandas==2.0.0, but you'll have pandas 2.2.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-utils 1.0.2 has requirement scikit-learn==1.2.1; python_version >= \"3.8\", but you'll have scikit-learn 1.3.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mlflow 2.6.0 has requirement pyarrow<13,>=4.0.0, but you'll have pyarrow 16.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mlflow 2.6.0 has requirement pytz<2024, but you'll have pytz 2024.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: openapi-schema-validator 0.6.2 has requirement jsonschema<5.0.0,>=4.19.1, but you'll have jsonschema 4.19.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement Flask==2.1.1; python_version >= \"3.7\", but you'll have flask 2.3.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement itsdangerous==2.0.1, but you'll have itsdangerous 2.1.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement Jinja2==3.0.3, but you'll have jinja2 3.1.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement matplotlib==3.6.0; python_version >= \"3.8\", but you'll have matplotlib 3.9.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-client 1.0.0 has requirement matplotlib==3.1.1, but you'll have matplotlib 3.9.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.7.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: jsonschema-path 0.3.2 has requirement referencing<0.32.0,>=0.28.0, but you'll have referencing 0.33.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: cloudpickle, async-timeout, attrs, frozenlist, multidict, idna, yarl, aiosignal, aiohttp, urllib3, charset-normalizer, certifi, requests, fsspec, typing-extensions, six, retrying, sqlparse, cachetools, wheel, setuptools, pyjwt, tomlkit, pycparser, cffi, cryptography, pyOpenSSL, pytz, asn1crypto, packaging, sortedcontainers, platformdirs, filelock, numpy, pyarrow, python-dateutil, tzdata, pandas, snowflake-connector-python, pyyaml, snowflake-snowpark-python, wrapt, jmespath, botocore, aioitertools, aiobotocore, s3fs, zipp, importlib-resources, scipy, xgboost, tenacity, plotly, graphviz, pillow, kiwisolver, pyparsing, contourpy, cycler, fonttools, matplotlib, catboost, threadpoolctl, joblib, scikit-learn, sniffio, exceptiongroup, anyio, pytimeparse, absl-py, snowflake-ml-python, fosforml\n",
      "Successfully installed absl-py-1.4.0 aiobotocore-2.13.1 aiohttp-3.9.5 aioitertools-0.11.0 aiosignal-1.3.1 anyio-3.7.1 asn1crypto-1.5.1 async-timeout-4.0.3 attrs-23.2.0 botocore-1.34.131 cachetools-5.3.3 catboost-1.2.5 certifi-2024.7.4 cffi-1.16.0 charset-normalizer-3.3.2 cloudpickle-2.2.1 contourpy-1.2.1 cryptography-42.0.8 cycler-0.12.1 exceptiongroup-1.2.2 filelock-3.15.4 fonttools-4.53.1 fosforml-1.0.9 frozenlist-1.4.1 fsspec-2023.12.2 graphviz-0.20.3 idna-3.7 importlib-resources-6.4.0 jmespath-1.0.1 joblib-1.4.2 kiwisolver-1.4.5 matplotlib-3.9.1 multidict-6.0.5 numpy-1.26.4 packaging-23.2 pandas-2.2.2 pillow-10.4.0 platformdirs-4.2.2 plotly-5.22.0 pyOpenSSL-24.1.0 pyarrow-16.1.0 pycparser-2.22 pyjwt-2.8.0 pyparsing-3.1.2 python-dateutil-2.9.0.post0 pytimeparse-1.1.8 pytz-2024.1 pyyaml-6.0.1 requests-2.32.3 retrying-1.3.4 s3fs-2023.12.2 scikit-learn-1.3.2 scipy-1.13.1 setuptools-70.3.0 six-1.16.0 sniffio-1.3.1 snowflake-connector-python-3.11.0 snowflake-ml-python-1.5.0 snowflake-snowpark-python-1.19.0 sortedcontainers-2.4.0 sqlparse-0.5.0 tenacity-8.5.0 threadpoolctl-3.5.0 tomlkit-0.13.0 typing-extensions-4.12.2 tzdata-2024.1 urllib3-1.26.19 wheel-0.43.0 wrapt-1.16.0 xgboost-1.7.6 yarl-1.9.4 zipp-3.19.2\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install fosforml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6b1e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fosforio\n",
      "  Downloading https://files.pythonhosted.org/packages/5b/98/b5e8b5febc280807ac2ebabd7d8f1e1986add138a393fd03647115a200ad/fosforio-1.0.2-py3-none-any.whl\n",
      "Collecting pandas==2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/7c/93b2b93ef90642a6a8ecf896d669f8ab5579b36fef7faca730b0c1da6034/pandas-2.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4MB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Using cached https://files.pythonhosted.org/packages/9c/3d/a121f284241f08268b21359bd425f7d4825cffc5ac5cd0e1b3d82ffd2b10/pytz-2024.1-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached https://files.pythonhosted.org/packages/65/58/f9c9e6be752e9fcb8b6a0ee9fb87e6e7a1f6bcab2cdc73f02bb7ba91ada0/tzdata-2024.1-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.20.3; python_version < \"3.10\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/d3/74e627205462a170f39e7d7ddd2b4166a0d8ab163377592c7f4fa935cc8c/numpy-2.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3MB)\n",
      "\u001b[K     |████████████████████████████████| 19.3MB 43.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six>=1.5\n",
      "  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl\n",
      "\u001b[31mERROR: scipy 1.12.0 has requirement numpy<1.29.0,>=1.22.4, but you'll have numpy 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mlflow 2.6.0 has requirement numpy<2, but you'll have numpy 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mlflow 2.6.0 has requirement pytz<2024, but you'll have pytz 2024.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: matplotlib 3.8.3 has requirement numpy<2,>=1.21, but you'll have numpy 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: contourpy 1.2.0 has requirement numpy<2.0,>=1.20, but you'll have numpy 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.5.0 has requirement numpy<2,>=1.23, but you'll have numpy 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: snowflake-ml-python 1.5.0 has requirement xgboost<2,>=1.7.3, but you'll have xgboost 2.0.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: fosforml 1.0.9 has requirement scikit-learn==1.3.2, but you'll have scikit-learn 1.2.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: openapi-schema-validator 0.6.2 has requirement jsonschema<5.0.0,>=4.19.1, but you'll have jsonschema 4.19.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: numba 0.58.1 has requirement numpy<1.27,>=1.22, but you'll have numpy 2.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement Flask==2.1.1; python_version >= \"3.7\", but you'll have flask 2.3.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement itsdangerous==2.0.1, but you'll have itsdangerous 2.1.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement Jinja2==3.0.3, but you'll have jinja2 3.1.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-serving 1.0.0 has requirement matplotlib==3.6.0; python_version >= \"3.8\", but you'll have matplotlib 3.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mosaic-ai-client 1.0.0 has requirement matplotlib==3.1.1, but you'll have matplotlib 3.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pytz, six, python-dateutil, tzdata, numpy, pandas, fosforio\n",
      "Successfully installed fosforio-1.0.2 numpy-2.0.0 pandas-2.0.0 python-dateutil-2.9.0.post0 pytz-2024.1 six-1.16.0 tzdata-2024.1\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/python_dateutil-2.9.0.post0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/pandas already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/tzdata already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/tzdata-2024.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/dateutil already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/six-1.16.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/pytz already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/pytz-2024.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /tmp/pip_packages/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install fosforio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! env | grep -i userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa851b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9345492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# project_id = str(uuid.uuid4())\n",
    "run_id = random.randint(100000, 999999)\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"fd0cf79c-8118-43a7-8f0b-059e8f78227a\".upper().replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FD0CF79C_8118_43A7_8F0B_059E8F78227A_exp_001_run_0001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c66b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_data = {\n",
    "    \"algorithm_category\": \"Snowparkml\",\n",
    "    \"algorithm_type\": \"Classification\",\n",
    "    \"algorithms\": [\n",
    "      {\n",
    "        \"algorithm_name\": \"snowflake.ml.modeling.xgboost.XGBClassifier\"\n",
    "      }\n",
    "    ],\n",
    "    \"dataset_name\": \"EMPLOYEE\",\n",
    "    \"experiment_id\": \"600b9f90-f431-4590-83c7-f38fe138dedc\",\n",
    "    \"experiment_name\": \"New_Experiment_05\",\n",
    "    \"monitor_run_id\": \"553620\",\n",
    "    \"project_id\": \"68bce134-6d7a-4e24-9591-86266438acf9\",\n",
    "    \"stored_procedure\": \"run_experiment\",\n",
    "    \"target_column\": \"LEAVEORNOT\"\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_data = '''{\n",
    "            \"monitor_run_id\": \"16e3f179-bb70-4632-9a39-9a6fac4a5581\",\n",
    "            \"project_id\": \"test12345\",\n",
    "            \"experiment_id\" : \"exp6789\", \n",
    "            \"experiment_name\":\"My_run_listing_exp\", \n",
    "            \"stored_procedure\":\"run_experiment\",\n",
    "            \"algorithm_category\":\"Snowparkml\",\n",
    "            \"algorithm_type\":\"Classification\",\n",
    "            \"dataset_name\":\"EMPLOYEE_10L\",\n",
    "            \"target_column\":\"LEAVEORNOT\",\n",
    "            \"algorithms\":[{\"algorithm_name\": \"snowflake.ml.modeling.tree.DecisionTreeClassifier\"}]\n",
    "        }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d621e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-1 (classification Airline Delay dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"classification\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.xgboost.XGBClassifier\": null}}, \n",
    "\"dataset\": \"AIRLINE_DEP_DELAY_10K\", \n",
    "\"target_column\": \"DEP_DEL15\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df37eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-2 (Regression Alcohol dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"regression\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.ensemble.RandomForestRegressor\": null}}, \n",
    "\"dataset\": \"ALCOHOL_DATA_10L\", \n",
    "\"target_column\": \"QUALITY\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-3 (classification Employee dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"classification\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.linear_model.LogisticRegression\": null}}, \n",
    "\"dataset\": \"EMPLOYEE_10L\", \n",
    "\"target_column\": \"LEAVEORNOT\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-4 (Regression Diamonds dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"regression\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.xgboost.XGBRegressor\": null}}, \n",
    "\"dataset\": \"DIAMONDS\", \n",
    "\"target_column\": \"PRICE\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8fc1b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm_category': 'Snowparkml',\n",
       " 'algorithm_type': 'Classification',\n",
       " 'algorithms': [{'algorithm_name': 'snowflake.ml.modeling.xgboost.XGBClassifier'}],\n",
       " 'dataset_name': 'EMPLOYEE',\n",
       " 'experiment_id': '600b9f90-f431-4590-83c7-f38fe138dedc',\n",
       " 'experiment_name': 'New_Experiment_05',\n",
       " 'monitor_run_id': '553620',\n",
       " 'project_id': '68bce134-6d7a-4e24-9591-86266438acf9',\n",
       " 'stored_procedure': 'run_experiment',\n",
       " 'target_column': 'LEAVEORNOT'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_exp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "\n",
    "session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101789b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection manager service url initialised to http://fdc-project-manager:80/project-manager\n",
      "If you need to update its value then update the variable CONNECTION_MANAGER_BASE_URL in os env.\n"
     ]
    }
   ],
   "source": [
    "from fosforio.manager import get_conn_details_from_ds_name\n",
    "from snowflake.snowpark.session import Session\n",
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e7bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage(session, stage_name=\"insight_exp\"):\n",
    "    try:\n",
    "        session.sql(f\"create or replace stage {stage_name}\").collect()\n",
    "        return f\"@{stage_name}\"\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake session\", ex)\n",
    "        raise ex\n",
    "\n",
    "        \n",
    "def get_session(dataset, project_id):\n",
    "    \"\"\"\n",
    "    Method creates snowflake session object.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_conn_details_from_ds_name(dataset, project_id)\n",
    "        region=conn[\"params\"][\"READER\"][\"region\"] if conn[\"params\"][\"READER\"][\"cloudPlatform\"] is None \\\n",
    "                    else conn[\"params\"][\"READER\"][\"region\"]+\".\"+conn[\"params\"][\"READER\"][\"cloudPlatform\"]\n",
    "        account = conn['params']['READER']['accountId'] if region is None \\\n",
    "                    else conn['params']['READER']['accountId']+\".\"+region\n",
    "        CONNECTION_PARAMETERS = {\n",
    "            \"account\": account,\n",
    "            \"user\":conn['params']['READER']['user'],\n",
    "            \"password\": conn['params']['READER']['password'],\n",
    "            \"role\": conn['params']['READER']['role'],\n",
    "            \"database\": conn['params']['READER']['database'],\n",
    "            \"warehouse\": conn['params']['READER']['wareHouse'],\n",
    "            \"schema\": conn['params']['READER']['schema']\n",
    "        }\n",
    "        return Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake session\", ex)\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec151ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(session: Session, exp_data: dict) -> list:\n",
    "    # Imports\n",
    "    from snowflake.ml.modeling.pipeline import Pipeline\n",
    "    from snowflake.ml.modeling.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "    from snowflake.ml.modeling.metrics import r2_score, accuracy_score, precision_score, roc_auc_score, \\\n",
    "        f1_score, recall_score, log_loss, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    from snowflake.snowpark.functions import col, is_null, regexp_replace, when, lit\n",
    "    from snowflake.snowpark.types import StringType\n",
    "    from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "    import importlib, sys, json, os, logging\n",
    "    from snowflake.ml.registry.registry import Registry\n",
    "    from fosforml import register_model\n",
    "    import os\n",
    "    \n",
    "    os.environ['RUN_ID'] = '927947'\n",
    "\n",
    "\n",
    "    # variable for holding logs\n",
    "    logs = []\n",
    "    \n",
    "    # function for accumulating logs\n",
    "    def log_message(level: str, message: str):\n",
    "        logs.append(f\"{level}: {message}\")\n",
    "    \n",
    "    \n",
    "    # Functions used in stored proc\n",
    "    def apply_data_cleansing(df):\n",
    "        \"\"\"\n",
    "        Method handles null values in snowpark dataframe.\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        :returns:\n",
    "        df_cleaned: dataframe after null handling\n",
    "        \"\"\"\n",
    "        # fillna - Unknown and 0\n",
    "        schema_fields = df.schema.fields\n",
    "        fill_values = {field.name: \"UNKNOWN\" if isinstance(field.datatype, StringType) else 0 for field in schema_fields}\n",
    "        df_cleaned = df.fillna(fill_values)\n",
    "        return df_cleaned\n",
    "\n",
    "\n",
    "    def get_feature_columns(df):\n",
    "        \"\"\"\n",
    "        Identifies the numerical and categorical features in dataset.\n",
    "        Identifies features for label encoding and one hot encoding\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        :returns:\n",
    "        categorical_features: list of non-numerical feature columns\n",
    "        numerical_features: list of numerical feature columns\n",
    "        le_column_features: list of feature label encoder columns\n",
    "        oh_column_features: list of feature one hot encoder columns\n",
    "        \"\"\"\n",
    "        schema_fields = df.schema.fields\n",
    "        features = df.columns\n",
    "        features.remove(exp_details.get(\"target_column\"))\n",
    "        df_schema = session.sql(f\"DESCRIBE TABLE {exp_details.get('dataset_name')}\").collect()\n",
    "        categorical_types = ['VARCHAR','CHAR','STRING','TEXT','BOOL']\n",
    "        categorical_features = []  \n",
    "        for row in df_schema:\n",
    "            for typ in categorical_types:\n",
    "                if typ in row['type']:\n",
    "                    categorical_features.append(row['name'])\n",
    "                    break\n",
    "        numerical_features = list(set(features) - set(categorical_features))\n",
    "        print(f\"numerical_features:  {numerical_features}\")\n",
    "        print(f\"categorical_features: {categorical_features}\")\n",
    "        log_message(\"INFO\",f\"numerical_features:  {numerical_features}\")\n",
    "        log_message(\"INFO\",f\"categorical_features: {categorical_features}\")\n",
    "        \n",
    "        \n",
    "        #identify columns for labelencoding and onehotencoding   \n",
    "        le_column_features = categorical_features\n",
    "        oh_column_features = []\n",
    "        if len(categorical_features) >= 1:\n",
    "            log_message(\"INFO\",f\"{categorical_features} columns are non numeric in feature dataset, encoding required.\")\n",
    "            for column in categorical_features:\n",
    "                if df.select(df[column]).distinct().count() < 10:\n",
    "                    oh_column_features.append(column)\n",
    "            print(f\"Columns identified to be encoded with label encoder: {le_column_features}\")\n",
    "            print(f\"Columns identified to be encoded with one hot encoder: {oh_column_features}\")\n",
    "            log_message(\"INFO\",f\"Columns identified to be encoded with label encoder: {le_column_features}\")\n",
    "            log_message(\"INFO\",f\"Columns identified to be encoded with one hot encoder: {oh_column_features}\")\n",
    "        return categorical_features, numerical_features, le_column_features, oh_column_features\n",
    "\n",
    "\n",
    "    def create_and_run_preprocessing(df, categorical_features, numerical_features, le_column_features, oh_column_features):\n",
    "        \"\"\"\n",
    "        Based on different features column input creates preprocessing steps and run it on input dataframe\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        categorical_features: list of non-numerical feature columns\n",
    "        numerical_features: list of numerical feature columns\n",
    "        le_column_features: list of feature label encoder columns\n",
    "        oh_column_features: list of feature one hot encoder columns\n",
    "        :returns:\n",
    "        df_train: preprocessed train dataset\n",
    "        df_test: preprocessed test dataset\n",
    "        \"\"\"\n",
    "        #pipeline steps \n",
    "        log_message(\"INFO\",\"Setting up preprocessing pipeline based on dataset\")\n",
    "        categorical_pp = {f'le_{column}':LabelEncoder(input_cols=column, output_cols=column) for column in le_column_features}\n",
    "        if len(oh_column_features)>0:\n",
    "            categorical_pp['oh_enc'] = OneHotEncoder(input_cols=oh_column_features, output_cols=oh_column_features, handle_unknown='ignore')\n",
    "        numerical_pp = {\n",
    "            'scaler': MinMaxScaler(input_cols=numerical_features, output_cols=numerical_features)\n",
    "        }\n",
    "        steps = [(key, categorical_pp[key]) for key in categorical_pp if categorical_pp[key]!=[]] + \\\n",
    "        [(key, numerical_pp[key]) for key in numerical_pp if numerical_features!=[]]\n",
    "        print(f\"Selected preprocesing steps: \\n{steps}\")\n",
    "        log_message(\"INFO\",f\"Selected preprocesing steps: \\n{steps}\")   \n",
    "            \n",
    "        # Run preprocessing pipeline steps\n",
    "        print(\"Running data preprocessing pipeline\")\n",
    "        log_message(\"INFO\",\"Running data preprocessing pipeline\") \n",
    "        df = Pipeline(steps=steps).fit(df).transform(df)\n",
    "        print(f\"Transformed dataset: \\n {df.show()}\")\n",
    "        log_message(\"INFO\",f\"Transformed dataset: \\n {df.show()}\")\n",
    "        df_train, df_test = df.random_split(weights=[0.8, 0.2], seed=0)\n",
    "        for col_name in df_train.schema.names:\n",
    "            new_col = col_name.replace('.','_')\n",
    "            df_train = df_train.withColumnRenamed(col_name, new_col)\n",
    "        for col_name in df_test.schema.names:\n",
    "            new_col = col_name.replace('.','_')\n",
    "            df_test = df_test.withColumnRenamed(col_name, new_col)\n",
    "        return df_train, df_test\n",
    "\n",
    "\n",
    "    def run_estimator(df_train, df_test, input_cols):\n",
    "        \"\"\"\n",
    "        trains on df_train and creates model object for given algorithm/estimator.\n",
    "        runs prediction function of model object on test dataset\n",
    "        :param:\n",
    "        df_train: input training dataframe\n",
    "        df_test: input test dataframe\n",
    "        input_cols: list of input feature names\n",
    "        :returns:\n",
    "        model: trained model object\n",
    "        df_pred: output predict dataframe\n",
    "        \"\"\"\n",
    "        for algorithm in exp_details.get(\"algorithms\"):\n",
    "            algorithm = algorithm[\"algorithm_name\"].rsplit('.', 1)\n",
    "            module = importlib.import_module(algorithm[0])\n",
    "            print(f\"----Running Algorithm {algorithm[1]}----\")\n",
    "            log_message(\"INFO\",f\"----Running Algorithm {algorithm[1]}----\")\n",
    "            attr = getattr(module, algorithm[1])\n",
    "            pipe = Pipeline(steps=[(\"algorithm\", attr(input_cols=input_cols\n",
    "                                                  , label_cols=[exp_details.get(\"target_column\")]\n",
    "                                                  , output_cols=['PREDICTIONS']))]\n",
    "                   )\n",
    "    \n",
    "            # Fit the pipeline\n",
    "            print(f\"Running model pipeline {algorithm[1]}\")\n",
    "            log_message(\"INFO\",f\"Running model pipeline {algorithm[1]}\")\n",
    "            model = pipe.fit(df_train)\n",
    "            print(dir(model))\n",
    "            # Test the model\n",
    "            print(\"Running prediction on model with test dataset\")\n",
    "            log_message(\"INFO\",\"Running prediction on model with test dataset\")\n",
    "            df_pred = model.predict(df_test)\n",
    "            return model, df_pred, algorithm[1]\n",
    "\n",
    "    \n",
    "    def try_or(fn):\n",
    "        try:\n",
    "            out = fn()\n",
    "            return out\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "        \n",
    "    def eval_metrics(df_pred):\n",
    "        log_message(\"INFO\",\"Generating Metrices\")\n",
    "        if exp_details.get(\"algorithm_type\") == 'Classification':\n",
    "            return {\n",
    "            'accuracy_score': try_or(lambda: accuracy_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'f1_score': try_or(lambda: f1_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'recall_score': try_or(lambda: recall_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'precision_score': try_or(lambda: precision_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'roc_auc': try_or(lambda: roc_auc_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_score_col_names='PREDICTIONS')),\n",
    "            'log_loss': try_or(lambda: log_loss(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS'))\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "            'mean_absolute_percentage_error': try_or(lambda: mean_absolute_percentage_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'r2_score': try_or(lambda: r2_score(df=df_pred, y_true_col_name=exp_details.get(\"target_column\"), y_pred_col_name='PREDICTIONS')),\n",
    "            'mean_absolute_error': try_or(lambda: mean_absolute_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'mean_squared_error': try_or(lambda: mean_squared_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS'))\n",
    "            }\n",
    "\n",
    "\n",
    "#     def register_model(model, metrics_info, estimator):\n",
    "#         log_message(\"INFO\",\"Started: Registering model on snowflake\")\n",
    "#         reg = Registry(session=session)\n",
    "        \n",
    "#         clean = lambda x : x.replace(\"-\",\"_\")\n",
    "#         project_id = clean(exp_details.get(\"project_id\"))\n",
    "#         exp_name = clean(exp_details.get(\"experiment_name\"))\n",
    "#         run_id = clean(exp_details.get(\"monitor_run_id\"))\n",
    "#         model_name = \"Experiment_\"+project_id+\"_FDC_\"+exp_name\n",
    "#         mv = reg.log_model(model=model,\n",
    "#                            model_name=model_name,\n",
    "#                            comment=exp_details.get(\"description\"),\n",
    "#                            version_name='V_'+exp_details.get(\"monitor_run_id\"),\n",
    "#                            python_version=\"3.9.19\",\n",
    "#                            conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "#                            metrics={\"model_metrics\": metrics_info,\n",
    "#                                     \"dataset_details\": {\n",
    "#                                         \"dataset_name\": exp_details.get(\"dataset_name\"),\n",
    "#                                         \"target_column\": exp_details.get(\"target_column\"),\n",
    "#                                     },\n",
    "#                                     \"hyper_parameters\": {'criterion': 'entropy', 'max_depth': None, 'max_features': 8, 'min_samples_leaf': 7},\n",
    "#                                      \"algorithm_type\": exp_details.get(\"algorithm_type\"),\n",
    "#                                      \"algorithm\": estimator,                                   \n",
    "#                                      \"experiment_name\": exp_details.get(\"experiment_name\"),\n",
    "#                                      \"source\": \"Experiment\",\n",
    "#                                      \"status\": \"Trained\",\n",
    "#                                      \"run_id\": exp_details.get(\"monitor_run_id\"),\n",
    "#                                      \"experiment_id\": exp_details.get(\"experiment_id\")})\n",
    "#         log_message(\"INFO\",\"Registeration of model completed!!!\")\n",
    "#         return model_name\n",
    "            \n",
    "            \n",
    "    def create_tags(session, exp_details):\n",
    "        for key in exp_details.keys():\n",
    "            tag = session.sql(f\"CREATE TAG IF NOT EXISTS {key}\")\n",
    "            tag.show()\n",
    "        if exp_details.get('algorithm_type')=='Classification':\n",
    "            metric_names=[\"accuracy_score\",\"precision_score\",\"recall_score\",\"f1_score\",\"roc_auc\",\"log_loss\"]\n",
    "        else:\n",
    "            metric_names=[\"r2_score\",\"mean_absolute_error\",\"mean_squared_error\",\"mean_absolute_percentage_error\"]\n",
    "        for name in metric_names:\n",
    "            tag = session.sql(f\"CREATE TAG IF NOT EXISTS {name}\")\n",
    "            tag.show()\n",
    "\n",
    "        \n",
    "    def set_tags(session, m_name, exp_details, metric_info):\n",
    "        for key, value in exp_details.items():\n",
    "            value = str(value).replace(\"'\",\"\\\"\")\n",
    "            tag = session.sql(f\"ALTER MODEL IF EXISTS {m_name} SET TAG {key}='{value}'\")\n",
    "            tag.show()\n",
    "        for key, value in metric_info.items():\n",
    "            value = str(value).replace(\"'\",\"\\\"\")\n",
    "            tag = session.sql(f\"ALTER MODEL IF EXISTS {m_name} SET TAG {key}='{value}'\")\n",
    "            tag.show()\n",
    "\n",
    "    try:\n",
    "        # loading experiment details\n",
    "        exp_details=exp_data\n",
    "        os.environ[\"RUN_ID\"] = os.environ['run_id'] = exp_data.get(\"monitor_run_id\")\n",
    "        os.environ[\"algorithm_type\"] = exp_data.get(\"algorithm_type\")\n",
    "        # os.environ[\"algorithm\"] = exp_data.get(\"algorithm\")\n",
    "        os.environ[\"experiment_name\"] = exp_data.get(\"experiment_name\", \"\")\n",
    "        os.environ[\"experiment_id\"] = exp_data.get(\"experiment_id\")\n",
    "        os.environ[\"PROJECT_ID\"] = exp_data.get(\"project_id\")\n",
    "        os.environ[\"userId\"] = 'ADITYASINGH'\n",
    "\n",
    "        # creating user tags if not exist\n",
    "        create_tags(session, exp_details)\n",
    "\n",
    "        # Reading dataset\n",
    "        log_message(\"INFO\",\"Reading dataset features\")\n",
    "        data = session.table(exp_details.get(\"dataset_name\"))\n",
    "\n",
    "        # fillna\n",
    "        data = apply_data_cleansing(data)\n",
    "\n",
    "        # Identify feature columns\n",
    "        categorical_features, numerical_features, le_column_features, oh_column_features = get_feature_columns(data)\n",
    "\n",
    "        # Based on feature, do preprocessing\n",
    "        data_train, data_test = create_and_run_preprocessing(data, categorical_features, numerical_features, le_column_features, oh_column_features)\n",
    "        \n",
    "        # Save Test and Train Dataset\n",
    "        data_train.write.mode(\"overwrite\").save_as_table(\"EMPLOYEE_TRAIN_CLEANED\")\n",
    "        data_train.write.mode(\"overwrite\").save_as_table(\"EMPLOYEE_TEST_CLEANED\")\n",
    "        \n",
    "        # Run model training and prediction\n",
    "        input_cols = data_train.columns\n",
    "        input_cols.remove(exp_details.get(\"target_column\"))\n",
    "        model, data_pred, estimator = run_estimator(data_train, data_test, input_cols)\n",
    "        \n",
    "        print(str(type(model)))\n",
    "\n",
    "        # Register model on snowflake registry\n",
    "        response = register_model(\n",
    "            model_obj=model,\n",
    "            session=session,\n",
    "            name=\"HR_CHURN_01\",\n",
    "            snowflake_df=data_pred,\n",
    "            dataset_name=exp_details.get(\"dataset_name\"),\n",
    "            dataset_source=\"SnowflakeDataset\",\n",
    "            description=\"This is a test model\",\n",
    "            flavour=\"snowflake\",\n",
    "            model_type=\"classification\",\n",
    "            prediction_column='PREDICTIONS',\n",
    "            conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "            source=\"Experiment\"\n",
    "        )\n",
    "        print(response)\n",
    "        log_message(\"INFO\", f\"{response}\")\n",
    "        return model, data_pred\n",
    "    except Exception as ex:\n",
    "        log_message(\"ERROR\", f\"{repr(ex)}\")\n",
    "        raise ex(\"Execution Logs: \"+\"\\n\".join(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfdd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "914d9fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Snowflake Session object...\n",
      "Session has been created !\n",
      "Creating stored procedure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following packages are not available in Snowflake: ['fosforml', 'snowflake-snowpark-python==1.19.0'].\n",
      "If you are adding package(s) unavailable in Snowflake, it is highly recommended that you include the 'cache_path' configuration parameter in order to reduce latency.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:13\u001b[0m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/stored_procedure.py:561\u001b[0m, in \u001b[0;36mStoredProcedureRegistration.register\u001b[0;34m(self, func, return_type, input_types, name, is_permanent, stage_location, imports, packages, replace, if_not_exists, parallel, execute_as, strict, external_access_integrations, secrets, comment, statement_params, source_code_display, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m native_app_params \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnative_app_params\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# register stored procedure\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_register_sp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstage_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_not_exists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexternal_access_integrations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_access_integrations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecrets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecrets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_as\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_as\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_call_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStoredProcedureRegistration.register\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_code_display\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_code_display\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43manonymous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manonymous\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_permanent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_permanent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# force_inline_code avoids uploading python file\u001b[39;49;00m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# when we know the code is not too large. This is useful\u001b[39;49;00m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in pandas API to create stored procedures not registered by users.\u001b[39;49;00m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_inline_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforce_inline_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnative_app_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnative_app_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/stored_procedure.py:797\u001b[0m, in \u001b[0;36mStoredProcedureRegistration._do_register_sp\u001b[0;34m(self, func, return_type, input_types, sp_name, stage_location, imports, packages, replace, if_not_exists, parallel, strict, source_code_display, statement_params, execute_as, anonymous, api_call_source, skip_upload_on_content_match, is_permanent, external_access_integrations, secrets, force_inline_code, comment, native_app_params)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;66;03m# Add in snowflake-snowpark-python if it is not already in the package list.\u001b[39;00m\n\u001b[1;32m    785\u001b[0m packages \u001b[38;5;241m=\u001b[39m add_snowpark_package_to_sproc_packages(\n\u001b[1;32m    786\u001b[0m     session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session,\n\u001b[1;32m    787\u001b[0m     packages\u001b[38;5;241m=\u001b[39mpackages,\n\u001b[1;32m    788\u001b[0m )\n\u001b[1;32m    790\u001b[0m (\n\u001b[1;32m    791\u001b[0m     handler,\n\u001b[1;32m    792\u001b[0m     code,\n\u001b[1;32m    793\u001b[0m     all_imports,\n\u001b[1;32m    794\u001b[0m     all_packages,\n\u001b[1;32m    795\u001b[0m     upload_file_stage_location,\n\u001b[1;32m    796\u001b[0m     custom_python_runtime_version_allowed,\n\u001b[0;32m--> 797\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_imports_and_packages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTempObjectType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPROCEDURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mudf_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstage_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimports\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_code_display\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_code_display\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_upload_on_content_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_upload_on_content_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_permanent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_permanent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_inline_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_inline_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m custom_python_runtime_version_allowed) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    815\u001b[0m     check_python_runtime_version(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39m_runtime_version_from_requirement\n\u001b[1;32m    817\u001b[0m     )\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/udf_utils.py:972\u001b[0m, in \u001b[0;36mresolve_imports_and_packages\u001b[0;34m(session, object_type, func, arg_names, udf_name, stage_location, imports, packages, parallel, is_pandas_udf, is_dataframe_input, max_batch_size, statement_params, source_code_display, skip_upload_on_content_match, is_permanent, force_inline_code)\u001b[0m\n\u001b[1;32m    969\u001b[0m     resolved_packages \u001b[38;5;241m=\u001b[39m resolve_packages_in_client_side_sandbox(packages\u001b[38;5;241m=\u001b[39mpackages)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# In any other scenario\u001b[39;00m\n\u001b[1;32m    971\u001b[0m     resolved_packages \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 972\u001b[0m         \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_packages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpackages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m            \u001b[49m\u001b[43minclude_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_pandas_udf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m packages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_resolve_packages(\n\u001b[1;32m    979\u001b[0m             [],\n\u001b[1;32m    980\u001b[0m             session\u001b[38;5;241m.\u001b[39m_packages,\n\u001b[1;32m    981\u001b[0m             validate_package\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    982\u001b[0m             include_pandas\u001b[38;5;241m=\u001b[39mis_pandas_udf,\n\u001b[1;32m    983\u001b[0m             statement_params\u001b[38;5;241m=\u001b[39mstatement_params,\n\u001b[1;32m    984\u001b[0m         )\n\u001b[1;32m    985\u001b[0m     )\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m     import_only_stage \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    989\u001b[0m         unwrap_stage_location_single_quote(stage_location)\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stage_location\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget_session_stage(statement_params\u001b[38;5;241m=\u001b[39mstatement_params)\n\u001b[1;32m    992\u001b[0m     )\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/session.py:1414\u001b[0m, in \u001b[0;36mSession._resolve_packages\u001b[0;34m(self, packages, existing_packages_dict, validate_package, include_pandas, statement_params)\u001b[0m\n\u001b[1;32m   1409\u001b[0m result_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1410\u001b[0m     existing_packages_dict \u001b[38;5;28;01mif\u001b[39;00m existing_packages_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   1411\u001b[0m )\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;66;03m# Retrieve list of dependencies that need to be added\u001b[39;00m\n\u001b[0;32m-> 1414\u001b[0m dependency_packages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dependency_packages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackage_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_package\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackage_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# Add dependency packages\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m package \u001b[38;5;129;01min\u001b[39;00m dependency_packages:\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/session.py:1345\u001b[0m, in \u001b[0;36mSession._get_dependency_packages\u001b[0;34m(self, package_dict, validate_package, package_table, current_packages, statement_params)\u001b[0m\n\u001b[1;32m   1339\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1340\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load environments from remote path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, creating a fresh \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1341\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment instead. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1342\u001b[0m             )\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dependency_packages:\n\u001b[0;32m-> 1345\u001b[0m         dependency_packages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_unsupported_packages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m            \u001b[49m\u001b[43munsupported_packages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpackage_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcurrent_packages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dependency_packages\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/session.py:1485\u001b[0m, in \u001b[0;36mSession._upload_unsupported_packages\u001b[0;34m(self, packages, package_table, package_dict)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(target):\n\u001b[1;32m   1483\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(target)\n\u001b[0;32m-> 1485\u001b[0m \u001b[43mpip_install_packages_to_target_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;66;03m# Create Requirement objects for packages installed, mapped to list of package files and folders.\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m downloaded_packages_dict \u001b[38;5;241m=\u001b[39m map_python_packages_to_files_and_folders(target)\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/packaging_utils.py:338\u001b[0m, in \u001b[0;36mpip_install_packages_to_target_folder\u001b[0;34m(packages, target, timeout)\u001b[0m\n\u001b[1;32m    328\u001b[0m pip_command: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    329\u001b[0m     [sys\u001b[38;5;241m.\u001b[39mexecutable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pip_executable \u001b[38;5;28;01melse\u001b[39;00m [pip_executable]\n\u001b[1;32m    330\u001b[0m )\n\u001b[1;32m    332\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m    333\u001b[0m     pip_command \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-t\u001b[39m\u001b[38;5;124m\"\u001b[39m, target, \u001b[38;5;241m*\u001b[39mpackages],\n\u001b[1;32m    334\u001b[0m     stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m    335\u001b[0m     stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m    336\u001b[0m     universal_newlines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    337\u001b[0m )\n\u001b[0;32m--> 338\u001b[0m stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m pip_install_result: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mreturncode\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stdout:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/subprocess.py:1134\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/subprocess.py:1995\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   1989\u001b[0m                         stdout, stderr,\n\u001b[1;32m   1990\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1991\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1993\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1995\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   1998\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   1999\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initilization\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print(\"Creating Snowflake Session object...\")\n",
    "# exp_details=json.loads(_exp_data)\n",
    "session = get_session(_exp_data.get(\"dataset_name\"),\"fd0cf79c-8118-43a7-8f0b-059e8f78227a\")\n",
    "stage = create_stage(session)\n",
    "print(\"Session has been created !\")\n",
    "print(\"Adding fosforml to stage\")\n",
    "session.sql(f\"PUT file://fosforml.zip {stage}\")\n",
    "print(\"Creating stored procedure...\")\n",
    "session.custom_package_usage_config['enabled'] = True\n",
    "session.custom_package_usage_config['force_push'] = True\n",
    "# session.custom_package_usage_config[\"cache_path\"] = stage+\"/run_experiment\"\n",
    "session.sproc.register(func=run_experiment,\n",
    "                       name=\"run_experiment\",\n",
    "                       packages=[\"fosforml\"],\n",
    "                       is_permanent=True,\n",
    "                       stage_location=stage,\n",
    "                       replace=True)\n",
    "print(\"Stored procedure has been created successfully!\")\n",
    "\n",
    "# tagging session\n",
    "print(\"Setting tag to session object: tag= \", \"110003\")\n",
    "session.query_tag=\"110003\"\n",
    "\n",
    "print(\"Executing Procedure\")\n",
    "procedure_response = session.call(\"run_experiment\", _exp_data)\n",
    "# procedure_response = run_experiment(session, _exp_data)\n",
    "print(\"Stored Procedure Executed Successfully !\")\n",
    "print(procedure_response)\n",
    "\n",
    "#Log in mlflow\n",
    "print(\"Logging in mlflow completed !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, df_pred = run_experiment(session, _exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc68c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_column = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = model.output_cols[0] if all([hasattr(model,\"output_cols\"),len(model.output_cols)>0]) else prediction_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71708e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all([hasattr(model,\"output_cols\"),len(model.output_cols)>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce66e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(type(model)).find(\"snowflake\") > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.output_cols)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843971b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(model,'n_classes_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b47ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef50d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model = model.to_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b297ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121260cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml import register_model\n",
    "response = register_model(\n",
    "            model_obj=model,\n",
    "            session=session,\n",
    "            name=\"First_FosforML\",\n",
    "            snowflake_df=df_pred,\n",
    "            dataset_name=_exp_data.get(\"dataset_name\"),\n",
    "            dataset_source=\"SnowflakeDataset\",\n",
    "            description=\"This is a test model\",\n",
    "            flavour=\"snowflake\",\n",
    "            model_type=\"classification\",\n",
    "            conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "            prediction_column=\"PREDICTIONS\",\n",
    "            source=\"Experiment\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc91bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml.model_manager import snowflakesession\n",
    "obj = snowflakesession()\n",
    "obj.connection_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c60f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg  =  Registry(\n",
    "    session= session,\n",
    "    database_name=\"FIRST_DB\",\n",
    "    schema_name=\"PUBLIC\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce565b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"SELECT TO_CHAR(CURRENT_TIMESTAMP,'YYYY-MM-DD HH24:MI:SS')\").to_pandas().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf8e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8db7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
