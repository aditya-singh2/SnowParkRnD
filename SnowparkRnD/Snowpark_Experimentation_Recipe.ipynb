{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install fosforml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! env | grep -i userId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa851b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9345492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537543\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# project_id = str(uuid.uuid4())\n",
    "run_id = random.randint(100000, 999999)\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"fd0cf79c-8118-43a7-8f0b-059e8f78227a\".upper().replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"FD0CF79C_8118_43A7_8F0B_059E8F78227A_exp_001_run_0001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c66b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_data = {\n",
    "    \"algorithm_category\": \"Snowparkml\",\n",
    "    \"algorithm_type\": \"Classification\",\n",
    "    \"algorithms\": [\n",
    "      {\n",
    "        \"algorithm_name\": \"snowflake.ml.modeling.xgboost.XGBClassifier\"\n",
    "      }\n",
    "    ],\n",
    "    \"dataset_name\": \"EMPLOYEE\",\n",
    "    \"experiment_id\": \"600b9f90-f431-4590-83c7-f38fe138dedc\",\n",
    "    \"experiment_name\": \"New_Experiment_05\",\n",
    "    \"monitor_run_id\": \"371498\",\n",
    "    \"project_id\": \"68bce134-6d7a-4e24-9591-86266438acf9\",\n",
    "    \"stored_procedure\": \"run_experiment\",\n",
    "    \"target_column\": \"LEAVEORNOT\"\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_data = '''{\n",
    "            \"monitor_run_id\": \"16e3f179-bb70-4632-9a39-9a6fac4a5581\",\n",
    "            \"project_id\": \"test12345\",\n",
    "            \"experiment_id\" : \"exp6789\", \n",
    "            \"experiment_name\":\"My_run_listing_exp\", \n",
    "            \"stored_procedure\":\"run_experiment\",\n",
    "            \"algorithm_category\":\"Snowparkml\",\n",
    "            \"algorithm_type\":\"Classification\",\n",
    "            \"dataset_name\":\"EMPLOYEE_10L\",\n",
    "            \"target_column\":\"LEAVEORNOT\",\n",
    "            \"algorithms\":[{\"algorithm_name\": \"snowflake.ml.modeling.tree.DecisionTreeClassifier\"}]\n",
    "        }'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d621e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-1 (classification Airline Delay dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"classification\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.xgboost.XGBClassifier\": null}}, \n",
    "\"dataset\": \"AIRLINE_DEP_DELAY_10K\", \n",
    "\"target_column\": \"DEP_DEL15\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df37eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-2 (Regression Alcohol dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"regression\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.ensemble.RandomForestRegressor\": null}}, \n",
    "\"dataset\": \"ALCOHOL_DATA_10L\", \n",
    "\"target_column\": \"QUALITY\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-3 (classification Employee dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"classification\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.linear_model.LogisticRegression\": null}}, \n",
    "\"dataset\": \"EMPLOYEE_10L\", \n",
    "\"target_column\": \"LEAVEORNOT\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-4 (Regression Diamonds dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"regression\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.xgboost.XGBRegressor\": null}}, \n",
    "\"dataset\": \"DIAMONDS\", \n",
    "\"target_column\": \"PRICE\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8fc1b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm_category': 'Snowparkml',\n",
       " 'algorithm_type': 'Classification',\n",
       " 'algorithms': [{'algorithm_name': 'snowflake.ml.modeling.xgboost.XGBClassifier'}],\n",
       " 'dataset_name': 'EMPLOYEE',\n",
       " 'experiment_id': '600b9f90-f431-4590-83c7-f38fe138dedc',\n",
       " 'experiment_name': 'New_Experiment_05',\n",
       " 'monitor_run_id': '371498',\n",
       " 'project_id': '68bce134-6d7a-4e24-9591-86266438acf9',\n",
       " 'stored_procedure': 'run_experiment',\n",
       " 'target_column': 'LEAVEORNOT'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_exp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf3ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml.model_manager.snowflakesession import get_session\n",
    "\n",
    "session = get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "101789b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fosforio.manager import get_conn_details_from_ds_name\n",
    "from snowflake.snowpark.session import Session\n",
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50e7bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage(session, stage_name=\"insight_exp\"):\n",
    "    try:\n",
    "        session.sql(f\"create or replace stage {stage_name}\").collect()\n",
    "        return f\"@{stage_name}\"\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake session\", ex)\n",
    "        raise ex\n",
    "\n",
    "        \n",
    "# def get_session(dataset, project_id):\n",
    "#     \"\"\"\n",
    "#     Method creates snowflake session object.\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         conn = get_conn_details_from_ds_name(dataset, project_id)\n",
    "#         region=conn[\"params\"][\"READER\"][\"region\"] if conn[\"params\"][\"READER\"][\"cloudPlatform\"] is None \\\n",
    "#                     else conn[\"params\"][\"READER\"][\"region\"]+\".\"+conn[\"params\"][\"READER\"][\"cloudPlatform\"]\n",
    "#         account = conn['params']['READER']['accountId'] if region is None \\\n",
    "#                     else conn['params']['READER']['accountId']+\".\"+region\n",
    "#         CONNECTION_PARAMETERS = {\n",
    "#             \"account\": account,\n",
    "#             \"user\":conn['params']['READER']['user'],\n",
    "#             \"password\": conn['params']['READER']['password'],\n",
    "#             \"role\": conn['params']['READER']['role'],\n",
    "#             \"database\": conn['params']['READER']['database'],\n",
    "#             \"warehouse\": conn['params']['READER']['wareHouse'],\n",
    "#             \"schema\": conn['params']['READER']['schema']\n",
    "#         }\n",
    "#         return Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "#     except Exception as ex:\n",
    "#         print(\"Error while creating snowflake session\", ex)\n",
    "#         raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec151ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(session: Session, exp_data: dict) -> list:\n",
    "    # Imports\n",
    "    from snowflake.ml.modeling.pipeline import Pipeline\n",
    "    from snowflake.ml.modeling.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "    from snowflake.ml.modeling.metrics import r2_score, accuracy_score, precision_score, roc_auc_score, \\\n",
    "        f1_score, recall_score, log_loss, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    from snowflake.snowpark.functions import col, is_null, regexp_replace, when, lit\n",
    "    from snowflake.snowpark.types import StringType\n",
    "    from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "    import importlib, sys, json, os, logging\n",
    "    from snowflake.ml.registry.registry import Registry\n",
    "    from fosforml import register_model\n",
    "    import os\n",
    "    \n",
    "    os.environ['RUN_ID'] = '927947'\n",
    "\n",
    "\n",
    "    # variable for holding logs\n",
    "    logs = []\n",
    "    \n",
    "    # function for accumulating logs\n",
    "    def log_message(level: str, message: str):\n",
    "        logs.append(f\"{level}: {message}\")\n",
    "    \n",
    "    \n",
    "    # Functions used in stored proc\n",
    "    def apply_data_cleansing(df):\n",
    "        \"\"\"\n",
    "        Method handles null values in snowpark dataframe.\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        :returns:\n",
    "        df_cleaned: dataframe after null handling\n",
    "        \"\"\"\n",
    "        # fillna - Unknown and 0\n",
    "        schema_fields = df.schema.fields\n",
    "        fill_values = {field.name: \"UNKNOWN\" if isinstance(field.datatype, StringType) else 0 for field in schema_fields}\n",
    "        df_cleaned = df.fillna(fill_values)\n",
    "        return df_cleaned\n",
    "\n",
    "\n",
    "    def get_feature_columns(df):\n",
    "        \"\"\"\n",
    "        Identifies the numerical and categorical features in dataset.\n",
    "        Identifies features for label encoding and one hot encoding\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        :returns:\n",
    "        categorical_features: list of non-numerical feature columns\n",
    "        numerical_features: list of numerical feature columns\n",
    "        le_column_features: list of feature label encoder columns\n",
    "        oh_column_features: list of feature one hot encoder columns\n",
    "        \"\"\"\n",
    "        schema_fields = df.schema.fields\n",
    "        features = df.columns\n",
    "        features.remove(exp_details.get(\"target_column\"))\n",
    "        df_schema = session.sql(f\"DESCRIBE TABLE {exp_details.get('dataset_name')}\").collect()\n",
    "        categorical_types = ['VARCHAR','CHAR','STRING','TEXT','BOOL']\n",
    "        categorical_features = []  \n",
    "        for row in df_schema:\n",
    "            for typ in categorical_types:\n",
    "                if typ in row['type']:\n",
    "                    categorical_features.append(row['name'])\n",
    "                    break\n",
    "        numerical_features = list(set(features) - set(categorical_features))\n",
    "        print(f\"numerical_features:  {numerical_features}\")\n",
    "        print(f\"categorical_features: {categorical_features}\")\n",
    "        log_message(\"INFO\",f\"numerical_features:  {numerical_features}\")\n",
    "        log_message(\"INFO\",f\"categorical_features: {categorical_features}\")\n",
    "        \n",
    "        \n",
    "        #identify columns for labelencoding and onehotencoding   \n",
    "        le_column_features = categorical_features\n",
    "        oh_column_features = []\n",
    "        if len(categorical_features) >= 1:\n",
    "            log_message(\"INFO\",f\"{categorical_features} columns are non numeric in feature dataset, encoding required.\")\n",
    "            for column in categorical_features:\n",
    "                if df.select(df[column]).distinct().count() < 10:\n",
    "                    oh_column_features.append(column)\n",
    "            print(f\"Columns identified to be encoded with label encoder: {le_column_features}\")\n",
    "            print(f\"Columns identified to be encoded with one hot encoder: {oh_column_features}\")\n",
    "            log_message(\"INFO\",f\"Columns identified to be encoded with label encoder: {le_column_features}\")\n",
    "            log_message(\"INFO\",f\"Columns identified to be encoded with one hot encoder: {oh_column_features}\")\n",
    "        return categorical_features, numerical_features, le_column_features, oh_column_features\n",
    "\n",
    "\n",
    "    def create_and_run_preprocessing(df, categorical_features, numerical_features, le_column_features, oh_column_features):\n",
    "        \"\"\"\n",
    "        Based on different features column input creates preprocessing steps and run it on input dataframe\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        categorical_features: list of non-numerical feature columns\n",
    "        numerical_features: list of numerical feature columns\n",
    "        le_column_features: list of feature label encoder columns\n",
    "        oh_column_features: list of feature one hot encoder columns\n",
    "        :returns:\n",
    "        df_train: preprocessed train dataset\n",
    "        df_test: preprocessed test dataset\n",
    "        \"\"\"\n",
    "        #pipeline steps \n",
    "        log_message(\"INFO\",\"Setting up preprocessing pipeline based on dataset\")\n",
    "        categorical_pp = {f'le_{column}':LabelEncoder(input_cols=column, output_cols=column) for column in le_column_features}\n",
    "        if len(oh_column_features)>0:\n",
    "            categorical_pp['oh_enc'] = OneHotEncoder(input_cols=oh_column_features, output_cols=oh_column_features, handle_unknown='ignore')\n",
    "        numerical_pp = {\n",
    "            'scaler': MinMaxScaler(input_cols=numerical_features, output_cols=numerical_features)\n",
    "        }\n",
    "        steps = [(key, categorical_pp[key]) for key in categorical_pp if categorical_pp[key]!=[]] + \\\n",
    "        [(key, numerical_pp[key]) for key in numerical_pp if numerical_features!=[]]\n",
    "        print(f\"Selected preprocesing steps: \\n{steps}\")\n",
    "        log_message(\"INFO\",f\"Selected preprocesing steps: \\n{steps}\")   \n",
    "            \n",
    "        # Run preprocessing pipeline steps\n",
    "        print(\"Running data preprocessing pipeline\")\n",
    "        log_message(\"INFO\",\"Running data preprocessing pipeline\") \n",
    "        df = Pipeline(steps=steps).fit(df).transform(df)\n",
    "        print(f\"Transformed dataset: \\n {df.show()}\")\n",
    "        log_message(\"INFO\",f\"Transformed dataset: \\n {df.show()}\")\n",
    "        df_train, df_test = df.random_split(weights=[0.8, 0.2], seed=0)\n",
    "        for col_name in df_train.schema.names:\n",
    "            new_col = col_name.replace('.','_')\n",
    "            df_train = df_train.withColumnRenamed(col_name, new_col)\n",
    "        for col_name in df_test.schema.names:\n",
    "            new_col = col_name.replace('.','_')\n",
    "            df_test = df_test.withColumnRenamed(col_name, new_col)\n",
    "        return df_train, df_test\n",
    "\n",
    "\n",
    "    def run_estimator(df_train, df_test, input_cols):\n",
    "        \"\"\"\n",
    "        trains on df_train and creates model object for given algorithm/estimator.\n",
    "        runs prediction function of model object on test dataset\n",
    "        :param:\n",
    "        df_train: input training dataframe\n",
    "        df_test: input test dataframe\n",
    "        input_cols: list of input feature names\n",
    "        :returns:\n",
    "        model: trained model object\n",
    "        df_pred: output predict dataframe\n",
    "        \"\"\"\n",
    "        for algorithm in exp_details.get(\"algorithms\"):\n",
    "            algorithm = algorithm[\"algorithm_name\"].rsplit('.', 1)\n",
    "            module = importlib.import_module(algorithm[0])\n",
    "            print(f\"----Running Algorithm {algorithm[1]}----\")\n",
    "            log_message(\"INFO\",f\"----Running Algorithm {algorithm[1]}----\")\n",
    "            attr = getattr(module, algorithm[1])\n",
    "            pipe = Pipeline(steps=[(\"algorithm\", attr(input_cols=input_cols\n",
    "                                                  , label_cols=[exp_details.get(\"target_column\")]\n",
    "                                                  , output_cols=['PREDICTIONS']))]\n",
    "                   )\n",
    "    \n",
    "            # Fit the pipeline\n",
    "            print(f\"Running model pipeline {algorithm[1]}\")\n",
    "            log_message(\"INFO\",f\"Running model pipeline {algorithm[1]}\")\n",
    "            model = pipe.fit(df_train)\n",
    "            print(dir(model))\n",
    "            # Test the model\n",
    "            print(\"Running prediction on model with test dataset\")\n",
    "            log_message(\"INFO\",\"Running prediction on model with test dataset\")\n",
    "            df_pred = model.predict(df_test)\n",
    "            return model, df_pred, algorithm[1]\n",
    "\n",
    "    \n",
    "    def try_or(fn):\n",
    "        try:\n",
    "            out = fn()\n",
    "            return out\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "        \n",
    "    def eval_metrics(df_pred):\n",
    "        log_message(\"INFO\",\"Generating Metrices\")\n",
    "        if exp_details.get(\"algorithm_type\") == 'Classification':\n",
    "            return {\n",
    "            'accuracy_score': try_or(lambda: accuracy_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'f1_score': try_or(lambda: f1_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'recall_score': try_or(lambda: recall_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'precision_score': try_or(lambda: precision_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'roc_auc': try_or(lambda: roc_auc_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_score_col_names='PREDICTIONS')),\n",
    "            'log_loss': try_or(lambda: log_loss(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS'))\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "            'mean_absolute_percentage_error': try_or(lambda: mean_absolute_percentage_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'r2_score': try_or(lambda: r2_score(df=df_pred, y_true_col_name=exp_details.get(\"target_column\"), y_pred_col_name='PREDICTIONS')),\n",
    "            'mean_absolute_error': try_or(lambda: mean_absolute_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'mean_squared_error': try_or(lambda: mean_squared_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS'))\n",
    "            }\n",
    "\n",
    "\n",
    "#     def register_model(model, metrics_info, estimator):\n",
    "#         log_message(\"INFO\",\"Started: Registering model on snowflake\")\n",
    "#         reg = Registry(session=session)\n",
    "        \n",
    "#         clean = lambda x : x.replace(\"-\",\"_\")\n",
    "#         project_id = clean(exp_details.get(\"project_id\"))\n",
    "#         exp_name = clean(exp_details.get(\"experiment_name\"))\n",
    "#         run_id = clean(exp_details.get(\"monitor_run_id\"))\n",
    "#         model_name = \"Experiment_\"+project_id+\"_FDC_\"+exp_name\n",
    "#         mv = reg.log_model(model=model,\n",
    "#                            model_name=model_name,\n",
    "#                            comment=exp_details.get(\"description\"),\n",
    "#                            version_name='V_'+exp_details.get(\"monitor_run_id\"),\n",
    "#                            python_version=\"3.9.19\",\n",
    "#                            conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "#                            metrics={\"model_metrics\": metrics_info,\n",
    "#                                     \"dataset_details\": {\n",
    "#                                         \"dataset_name\": exp_details.get(\"dataset_name\"),\n",
    "#                                         \"target_column\": exp_details.get(\"target_column\"),\n",
    "#                                     },\n",
    "#                                     \"hyper_parameters\": {'criterion': 'entropy', 'max_depth': None, 'max_features': 8, 'min_samples_leaf': 7},\n",
    "#                                      \"algorithm_type\": exp_details.get(\"algorithm_type\"),\n",
    "#                                      \"algorithm\": estimator,                                   \n",
    "#                                      \"experiment_name\": exp_details.get(\"experiment_name\"),\n",
    "#                                      \"source\": \"Experiment\",\n",
    "#                                      \"status\": \"Trained\",\n",
    "#                                      \"run_id\": exp_details.get(\"monitor_run_id\"),\n",
    "#                                      \"experiment_id\": exp_details.get(\"experiment_id\")})\n",
    "#         log_message(\"INFO\",\"Registeration of model completed!!!\")\n",
    "#         return model_name\n",
    "            \n",
    "            \n",
    "    def create_tags(session, exp_details):\n",
    "        for key in exp_details.keys():\n",
    "            tag = session.sql(f\"CREATE TAG IF NOT EXISTS {key}\")\n",
    "            tag.show()\n",
    "        if exp_details.get('algorithm_type')=='Classification':\n",
    "            metric_names=[\"accuracy_score\",\"precision_score\",\"recall_score\",\"f1_score\",\"roc_auc\",\"log_loss\"]\n",
    "        else:\n",
    "            metric_names=[\"r2_score\",\"mean_absolute_error\",\"mean_squared_error\",\"mean_absolute_percentage_error\"]\n",
    "        for name in metric_names:\n",
    "            tag = session.sql(f\"CREATE TAG IF NOT EXISTS {name}\")\n",
    "            tag.show()\n",
    "\n",
    "        \n",
    "    def set_tags(session, m_name, exp_details, metric_info):\n",
    "        for key, value in exp_details.items():\n",
    "            value = str(value).replace(\"'\",\"\\\"\")\n",
    "            tag = session.sql(f\"ALTER MODEL IF EXISTS {m_name} SET TAG {key}='{value}'\")\n",
    "            tag.show()\n",
    "        for key, value in metric_info.items():\n",
    "            value = str(value).replace(\"'\",\"\\\"\")\n",
    "            tag = session.sql(f\"ALTER MODEL IF EXISTS {m_name} SET TAG {key}='{value}'\")\n",
    "            tag.show()\n",
    "\n",
    "    try:\n",
    "        # loading experiment details\n",
    "        exp_details=exp_data\n",
    "        os.environ[\"RUN_ID\"] = os.environ['run_id'] = exp_data.get(\"monitor_run_id\")\n",
    "        os.environ[\"algorithm_type\"] = exp_data.get(\"algorithm_type\")\n",
    "        # os.environ[\"algorithm\"] = exp_data.get(\"algorithm\")\n",
    "        os.environ[\"experiment_name\"] = exp_data.get(\"experiment_name\", \"\")\n",
    "        os.environ[\"experiment_id\"] = exp_data.get(\"experiment_id\")\n",
    "        os.environ[\"PROJECT_ID\"] = exp_data.get(\"project_id\")\n",
    "        os.environ[\"userId\"] = 'ADITYASINGH'\n",
    "\n",
    "        # creating user tags if not exist\n",
    "        create_tags(session, exp_details)\n",
    "\n",
    "        # Reading dataset\n",
    "        log_message(\"INFO\",\"Reading dataset features\")\n",
    "        data = session.table(exp_details.get(\"dataset_name\"))\n",
    "\n",
    "        # fillna\n",
    "        data = apply_data_cleansing(data)\n",
    "\n",
    "        # Identify feature columns\n",
    "        categorical_features, numerical_features, le_column_features, oh_column_features = get_feature_columns(data)\n",
    "\n",
    "        # Based on feature, do preprocessing\n",
    "        data_train, data_test = create_and_run_preprocessing(data, categorical_features, numerical_features, le_column_features, oh_column_features)\n",
    "        \n",
    "        # Save Test and Train Dataset\n",
    "        data_train.write.mode(\"overwrite\").save_as_table(\"EMPLOYEE_TRAIN_CLEANED\")\n",
    "        data_train.write.mode(\"overwrite\").save_as_table(\"EMPLOYEE_TEST_CLEANED\")\n",
    "        \n",
    "        # Run model training and prediction\n",
    "        input_cols = data_train.columns\n",
    "        input_cols.remove(exp_details.get(\"target_column\"))\n",
    "        model, data_pred, estimator = run_estimator(data_train, data_test, input_cols)\n",
    "        \n",
    "        print(str(type(model)))\n",
    "\n",
    "        # Register model on snowflake registry\n",
    "        response = register_model(\n",
    "            model_obj=model,\n",
    "            session=session,\n",
    "            name=\"HR_CHURN_01\",\n",
    "            snowflake_df=data_pred,\n",
    "            dataset_name=exp_details.get(\"dataset_name\"),\n",
    "            dataset_source=\"SnowflakeDataset\",\n",
    "            description=\"This is a test model\",\n",
    "            flavour=\"snowflake\",\n",
    "            model_type=\"classification\",\n",
    "            prediction_column='PREDICTIONS',\n",
    "            conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "            source=\"Experiment\"\n",
    "        )\n",
    "        print(response)\n",
    "        log_message(\"INFO\", f\"{response}\")\n",
    "        return model, data_pred\n",
    "    except Exception as ex:\n",
    "        log_message(\"ERROR\", f\"{repr(ex)}\")\n",
    "        raise ex(\"Execution Logs: \"+\"\\n\".join(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "914d9fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Snowflake Session object...\n",
      "Session has been created !\n",
      "Creating stored procedure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following packages are not available in Snowflake: ['fosforml', 'snowflake-snowpark-python==1.19.0'].\n",
      "If you are adding package(s) unavailable in Snowflake, it is highly recommended that you include the 'cache_path' configuration parameter in order to reduce latency.\n",
      "ERROR: refractml 1.0.3 has requirement cloudpickle==1.6.0, but you'll have cloudpickle 2.2.1 which is incompatible.\n",
      "ERROR: refractml 1.0.3 has requirement PyYAML==6.0, but you'll have pyyaml 6.0.1 which is incompatible.\n",
      "ERROR: refractml 1.0.3 has requirement urllib3==1.26.15, but you'll have urllib3 1.26.19 which is incompatible.\n",
      "ERROR: refractio 2.1.5.2 has requirement pandas==2.0.0, but you'll have pandas 2.2.2 which is incompatible.\n",
      "ERROR: mosaic-utils 1.0.2 has requirement scikit-learn==1.2.1; python_version >= \"3.8\", but you'll have scikit-learn 1.3.2 which is incompatible.\n",
      "ERROR: mlflow 2.6.0 has requirement pyarrow<13,>=4.0.0, but you'll have pyarrow 16.1.0 which is incompatible.\n",
      "ERROR: mlflow 2.6.0 has requirement pytz<2024, but you'll have pytz 2024.1 which is incompatible.\n",
      "ERROR: openapi-schema-validator 0.6.2 has requirement jsonschema<5.0.0,>=4.19.1, but you'll have jsonschema 4.19.0 which is incompatible.\n",
      "ERROR: mosaic-ai-serving 1.0.0 has requirement Flask==2.1.1; python_version >= \"3.7\", but you'll have flask 2.3.3 which is incompatible.\n",
      "ERROR: mosaic-ai-serving 1.0.0 has requirement itsdangerous==2.0.1, but you'll have itsdangerous 2.1.2 which is incompatible.\n",
      "ERROR: mosaic-ai-serving 1.0.0 has requirement Jinja2==3.0.3, but you'll have jinja2 3.1.3 which is incompatible.\n",
      "ERROR: mosaic-ai-serving 1.0.0 has requirement matplotlib==3.6.0; python_version >= \"3.8\", but you'll have matplotlib 3.9.1 which is incompatible.\n",
      "ERROR: mosaic-ai-client 1.0.0 has requirement matplotlib==3.1.1, but you'll have matplotlib 3.9.1 which is incompatible.\n",
      "ERROR: jupyterlab 3.2.4 has requirement jupyter-server~=1.4, but you'll have jupyter-server 2.7.3 which is incompatible.\n",
      "ERROR: jsonschema-path 0.3.2 has requirement referencing<0.32.0,>=0.28.0, but you'll have referencing 0.33.0 which is incompatible.\n",
      "WARNING: You are using pip version 19.3.1; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\n",
      "Package fonttools(version 4.53.1) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package wrapt(version 1.16.0) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package matplotlib(version 3.9.1) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package cryptography(version 42.0.8) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package scikit-learn(version 1.3.2) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package multidict(version 6.0.5) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package pillow(version 10.4.0) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package catboost(version 1.2.5) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package yarl(version 1.9.4) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package contourpy(version 1.2.1) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package kiwisolver(version 1.4.5) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package frozenlist(version 1.4.1) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package pyarrow(version 16.1.0) contains native code, switching to latest available version in Snowflake instead.\n",
      "Package charset-normalizer(version 3.3.2) contains native code, switching to latest available version in Snowflake instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored procedure has been created successfully!\n",
      "Setting tag to session object: tag=  110003\n",
      "Executing Procedure\n"
     ]
    },
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1304): 01b5a80b-0000-7420-0000-576d07634652: 100357 (P0000): Python Interpreter Error:\nsnowflake.connector.errors.ProgrammingError: 251005: User is empty in function RUN_EXPERIMENT with handler udf_py_1881034442.compute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:26\u001b[0m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/session.py:3079\u001b[0m, in \u001b[0;36mSession.call\u001b[0;34m(self, sproc_name, statement_params, log_on_exception, *args)\u001b[0m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3032\u001b[0m     sproc_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3035\u001b[0m     log_on_exception: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   3036\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   3037\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls a stored procedure by name.\u001b[39;00m\n\u001b[1;32m   3038\u001b[0m \n\u001b[1;32m   3039\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3077\u001b[0m \u001b[38;5;124;03m        <BLANKLINE>\u001b[39;00m\n\u001b[1;32m   3078\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3079\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m        \u001b[49m\u001b[43msproc_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/session.py:3125\u001b[0m, in \u001b[0;36mSession._call\u001b[0;34m(self, sproc_name, statement_params, is_return_table, log_on_exception, *args)\u001b[0m\n\u001b[1;32m   3123\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql(query)\n\u001b[1;32m   3124\u001b[0m set_api_call_source(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession.call\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/telemetry.py:145\u001b[0m, in \u001b[0;36mdf_collect_api_telemetry.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mquery_history() \u001b[38;5;28;01mas\u001b[39;00m query_history:\n\u001b[0;32m--> 145\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     plan \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_select_statement \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_plan\n\u001b[1;32m    147\u001b[0m     api_calls \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;241m*\u001b[39mplan\u001b[38;5;241m.\u001b[39mapi_calls,\n\u001b[1;32m    149\u001b[0m         {TelemetryField\u001b[38;5;241m.\u001b[39mNAME\u001b[38;5;241m.\u001b[39mvalue: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    150\u001b[0m     ]\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/dataframe.py:597\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self, statement_params, block, log_on_exception, case_sensitive)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Executes the query representing this DataFrame and returns the result as a\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03mlist of :class:`Row` objects.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;124;03m    :meth:`collect_nowait()`\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m open_telemetry_context_manager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect, \u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_collect_with_tag_no_telemetry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/dataframe.py:645\u001b[0m, in \u001b[0;36mDataFrame._internal_collect_with_tag_no_telemetry\u001b[0;34m(self, statement_params, block, data_type, log_on_exception, case_sensitive)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_collect_with_tag_no_telemetry\u001b[39m(\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# we should always call this method instead of collect(), to make sure the\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# query tag is set properly.\u001b[39;00m\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_statement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_or_update_statement_params_with_query_tag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_statement_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_tag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[43mSKIP_LEVELS_THREE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/server_connection.py:513\u001b[0m, in \u001b[0;36mServerConnection.execute\u001b[0;34m(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    504\u001b[0m     is_in_stored_procedure()\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    508\u001b[0m     )\n\u001b[1;32m    509\u001b[0m ):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsync query is not supported in stored procedure yet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m     )\n\u001b[0;32m--> 513\u001b[0m result_set, result_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result_set\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:191\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     ne \u001b[38;5;241m=\u001b[39m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[1;32m    189\u001b[0m         e\n\u001b[1;32m    190\u001b[0m     )\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ne\u001b[38;5;241m.\u001b[39mwith_traceback(tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:122\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m snowflake\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mProgrammingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    124\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/server_connection.py:617\u001b[0m, in \u001b[0;36mServerConnection.get_result_set\u001b[0;34m(self, plan, to_pandas, to_iter, block, data_type, log_on_exception, case_sensitive, ignore_results, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m holder, id_ \u001b[38;5;129;01min\u001b[39;00m placeholders\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    616\u001b[0m     final_query \u001b[38;5;241m=\u001b[39m final_query\u001b[38;5;241m.\u001b[39mreplace(holder, id_)\n\u001b[0;32m--> 617\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_ddl_on_temp_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_last\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_job_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_on_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcase_sensitive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcase_sensitive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m placeholders[query\u001b[38;5;241m.\u001b[39mquery_id_place_holder] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    632\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last \u001b[38;5;28;01melse\u001b[39;00m result\u001b[38;5;241m.\u001b[39mquery_id\n\u001b[1;32m    633\u001b[0m )\n\u001b[1;32m    634\u001b[0m result_meta \u001b[38;5;241m=\u001b[39m get_new_description(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cursor)\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/server_connection.py:123\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[1;32m    120\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/server_connection.py:117\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReauthenticationRequest \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[38;5;241m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[1;32m    120\u001b[0m         ex\u001b[38;5;241m.\u001b[39mcause\n\u001b[1;32m    121\u001b[0m     )\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/server_connection.py:418\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, ignore_results, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m         query_id_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to execute query\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_id_log\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/server_connection.py:403\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, log_on_exception, case_sensitive, params, num_statements, ignore_results, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_statement_params\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNOWPARK_SKIP_TXN_COMMIT_IN_DDL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m--> 403\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_and_notify_query_listener\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecute query [queryID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_cursor\u001b[38;5;241m.\u001b[39msfqid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/snowpark/_internal/server_connection.py:354\u001b[0m, in \u001b[0;36mServerConnection.execute_and_notify_query_listener\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_and_notify_query_listener\u001b[39m(\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SnowflakeCursor:\n\u001b[0;32m--> 354\u001b[0m     results_cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnotify_query_listeners(\n\u001b[1;32m    356\u001b[0m         QueryRecord(results_cursor\u001b[38;5;241m.\u001b[39msfqid, results_cursor\u001b[38;5;241m.\u001b[39mquery)\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results_cursor\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/cursor.py:1080\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, _skip_upload_on_content_match, file_stream, num_statements)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     is_integrity_error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1077\u001b[0m         code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100072\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1078\u001b[0m     )  \u001b[38;5;66;03m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m IntegrityError \u001b[38;5;28;01mif\u001b[39;00m is_integrity_error \u001b[38;5;28;01melse\u001b[39;00m ProgrammingError\n\u001b[0;32m-> 1080\u001b[0m     \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/errors.py:290\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    269\u001b[0m     connection: SnowflakeConnection \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     error_value: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     handed_over \u001b[38;5;241m=\u001b[39m \u001b[43mError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhand_to_other_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handed_over:\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Error\u001b[38;5;241m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    298\u001b[0m             error_class,\n\u001b[1;32m    299\u001b[0m             error_value,\n\u001b[1;32m    300\u001b[0m         )\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/errors.py:345\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 345\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrorhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/pip_packages/snowflake/connector/errors.py:221\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    219\u001b[0m errno \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrno\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m done_format_msg \u001b[38;5;241m=\u001b[39m error_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone_format_msg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error_class(\n\u001b[1;32m    222\u001b[0m     msg\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    223\u001b[0m     errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m errno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(errno),\n\u001b[1;32m    224\u001b[0m     sqlstate\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlstate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    225\u001b[0m     sfqid\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msfqid\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    226\u001b[0m     query\u001b[38;5;241m=\u001b[39merror_value\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    227\u001b[0m     done_format_msg\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m done_format_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(done_format_msg)\n\u001b[1;32m    229\u001b[0m     ),\n\u001b[1;32m    230\u001b[0m     connection\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    231\u001b[0m     cursor\u001b[38;5;241m=\u001b[39mcursor,\n\u001b[1;32m    232\u001b[0m )\n",
      "\u001b[0;31mSnowparkSQLException\u001b[0m: (1304): 01b5a80b-0000-7420-0000-576d07634652: 100357 (P0000): Python Interpreter Error:\nsnowflake.connector.errors.ProgrammingError: 251005: User is empty in function RUN_EXPERIMENT with handler udf_py_1881034442.compute"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initilization\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print(\"Creating Snowflake Session object...\")\n",
    "# exp_details=json.loads(_exp_data)\n",
    "# session = get_session(_exp_data.get(\"dataset_name\"),\"fd0cf79c-8118-43a7-8f0b-059e8f78227a\")\n",
    "stage = create_stage(session)\n",
    "print(\"Session has been created !\")\n",
    "\n",
    "print(\"Creating stored procedure...\")\n",
    "session.custom_package_usage_config['enabled'] = True\n",
    "session.custom_package_usage_config['force_push'] = True\n",
    "# session.custom_package_usage_config[\"cache_path\"] = stage\n",
    "session.sproc.register(func=run_experiment,\n",
    "                       name=\"run_experiment\",\n",
    "                       packages=[\"fosforml\"],\n",
    "                       is_permanent=True,\n",
    "                       stage_location=stage,\n",
    "                       replace=True)\n",
    "print(\"Stored procedure has been created successfully!\")\n",
    "\n",
    "# tagging session\n",
    "print(\"Setting tag to session object: tag= \", \"110003\")\n",
    "session.query_tag=\"110003\"\n",
    "\n",
    "print(\"Executing Procedure\")\n",
    "procedure_response = session.call(\"run_experiment\", _exp_data)\n",
    "# procedure_response = run_experiment(session, _exp_data)\n",
    "print(\"Stored Procedure Executed Successfully !\")\n",
    "print(procedure_response)\n",
    "\n",
    "#Log in mlflow\n",
    "print(\"Logging in mlflow completed !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d6c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, df_pred = run_experiment(session, _exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc68c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_column = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = model.output_cols[0] if all([hasattr(model,\"output_cols\"),len(model.output_cols)>0]) else prediction_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71708e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "all([hasattr(model,\"output_cols\"),len(model.output_cols)>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce66e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(type(model)).find(\"snowflake\") > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a0553",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.output_cols)>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843971b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(model,'n_classes_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b47ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef50d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_model = model.to_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b297ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121260cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml import register_model\n",
    "response = register_model(\n",
    "            model_obj=model,\n",
    "            session=session,\n",
    "            name=\"First_FosforML\",\n",
    "            snowflake_df=df_pred,\n",
    "            dataset_name=_exp_data.get(\"dataset_name\"),\n",
    "            dataset_source=\"SnowflakeDataset\",\n",
    "            description=\"This is a test model\",\n",
    "            flavour=\"snowflake\",\n",
    "            model_type=\"classification\",\n",
    "            conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "            prediction_column=\"PREDICTIONS\",\n",
    "            source=\"Experiment\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b06a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc91bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fosforml.model_manager import snowflakesession\n",
    "obj = snowflakesession()\n",
    "obj.connection_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d3a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c60f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg  =  Registry(\n",
    "    session= session,\n",
    "    database_name=\"FIRST_DB\",\n",
    "    schema_name=\"PUBLIC\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce565b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"SELECT TO_CHAR(CURRENT_TIMESTAMP,'YYYY-MM-DD HH24:MI:SS')\").to_pandas().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17f9393",
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf8e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8db7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
