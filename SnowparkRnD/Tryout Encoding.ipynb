{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "exp_data = '{\"name\": \"moremetrices01\", \"algo_details\": {\"snowflake.ml.modeling.ensemble.GradientBoostingClassifier\": null}, \"id\": \"367\", \"dataset\": \"AIRLINE_DEP_DELAY_10K\", \"target_column\": \"DEP_DEL15\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9aae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, sys, os\n",
    "from snowflake.snowpark.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58607978",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_PARAMETERS = {\n",
    "    \"account\": \"ug94937.us-east4.gcp\",\n",
    "    \"user\":\"ADITYASINGH\",\n",
    "    \"password\": os.environ.get('SF_Password'),\n",
    "    \"role\": \"ADITYASINGH\",\n",
    "    \"database\": \"FIRST_DB\",\n",
    "    \"warehouse\": \"FOSFOR_INSIGHT_WH\",\n",
    "    \"schema\": \"PUBLIC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage(session, stage_name=\"demo\"):\n",
    "    try:\n",
    "        session.sql(f\"create or replace stage {stage_name}\").collect()\n",
    "        return f\"@{stage_name}\"\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake session\", ex)\n",
    "        raise ex\n",
    "\n",
    "def get_session():\n",
    "    \"\"\"\n",
    "    Method creates snowflake session object.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake session\", ex)\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3baa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ml_models(session: Session, exp_data: str) -> list:\n",
    "    # variable for holding logs\n",
    "    logs = []\n",
    "    \n",
    "    # function for accumulating logs\n",
    "    def log_message(level: str, message: str):\n",
    "        logs.append(f\"{level}: {message}\")\n",
    "        \n",
    "    from snowflake.ml.modeling.pipeline import Pipeline\n",
    "    from snowflake.ml.modeling.preprocessing import MinMaxScaler, OrdinalEncoder, OneHotEncoder\n",
    "    from snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_error, \\\n",
    "         r2_score, accuracy_score, precision_score, roc_auc_score, f1_score, recall_score\n",
    "\n",
    "    from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "    import importlib, sys, json\n",
    "    import snowpark\n",
    "\n",
    "    log_message(\"INFO\",\"Starting Experiment Recipe Execution\")\n",
    "    \n",
    "    def encoding(df, target_column):\n",
    "        df_target = df[[target_column]]\n",
    "        le_target = None\n",
    "        # Target column validation and encoding\n",
    "        if df.dtypes[target_column].name in ['object', 'bool']:\n",
    "            print(f\"target_column is of {df.dtypes[target_column].name} datatype, encoding required.\")\n",
    "            le_target = LabelEncoder()\n",
    "            df_target[target_column] = pd.DataFrame(le_target.fit_transform(df_target[target_column].astype(str)))\n",
    "            print(f\"Target column label encoded {df_target[target_column]}, object: {le_target}\")\n",
    "    \n",
    "        # Feature column validation and encoding\n",
    "        df_feature = df.drop(target_column, axis=1)\n",
    "        non_numeric_cols = df_feature.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "        le_dict_feature = {}\n",
    "        le_column_feature = []\n",
    "        oh_column_feature = []\n",
    "        oh_enc_feature = None\n",
    "        if len(non_numeric_cols) >= 1:\n",
    "        print(f\"{non_numeric_cols} columns are non numeric in feature dataset, encoding required.\")\n",
    "        for col in non_numeric_cols:\n",
    "            if df_feature[col].nunique() >= 10:\n",
    "                le_column_feature.append(col)\n",
    "            else:\n",
    "                oh_column_feature.append(col)\n",
    "\n",
    "        print(f\"Columns identified to be encoded with label encoder: {le_column_feature}\\n\"\n",
    "              f\"Columns identified to be encoded with one hot encoder: {oh_column_feature}\")\n",
    "\n",
    "        # columns to be label encoded\n",
    "        if len(le_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            for col in le_column_feature:\n",
    "                le_dict_feature[col] = LabelEncoder()\n",
    "                df_feature[col] = le_dict_feature[col].fit_transform(df_feature[col].astype(str))\n",
    "                print(f\"{col} column label encoded {df_feature[col]}, object: {le_dict_feature[col]}\")\n",
    "\n",
    "        # columns to be one hot encoded\n",
    "        if len(oh_column_feature) == 0:\n",
    "            df_feature = df_feature\n",
    "        else:\n",
    "            unique_combinations = pd.get_dummies(df_feature[oh_column_feature])\n",
    "            unique_combinations_list = unique_combinations.columns.tolist()\n",
    "            oh_enc_feature = OneHotEncoder()\n",
    "            oh_encoded_array = oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray() if len(\n",
    "                oh_column_feature) > 1 else oh_enc_feature.fit_transform(df_feature[oh_column_feature]).toarray()\n",
    "            df_oh_enc = pd.DataFrame(oh_encoded_array, columns=unique_combinations_list)\n",
    "            df_feature = df_feature.drop(columns=oh_column_feature)\n",
    "            df_feature = df_feature.join(df_oh_enc)\n",
    "            print(f\"new one hot encoded df: {oh_encoded_array}\\n\"\n",
    "                  f\"one hot encoder object: {oh_enc_feature}\\n\")\n",
    "        print(f\"final feature df created: {df_feature}\")\n",
    "    return df_target, le_target, df_feature, le_dict_feature, oh_enc_feature, le_column_feature, oh_column_feature\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Experiment details\n",
    "    exp_details=json.loads(exp_data)\n",
    "    \n",
    "    # Read dataset, Random split\n",
    "    log_message(\"INFO\",\"Reading and Identifing dataset features\")\n",
    "    data = session.table(exp_details.get(\"dataset\"))\n",
    "    \n",
    "    # Data Preprocessing: Validating and encoding the data if required and imputing null values.\n",
    "    data = data.select([col for col in data.columns], \n",
    "                      *[snowpark.functions.coalesce(col, snowpark.functions.lag(col)) for col in data.columns])\n",
    "    data = data.select([col for col in data.columns], \n",
    "                      *[snowpark.functions.coalesce(col, snowpark.functions.lead(col)) for col in data.columns])\n",
    "    \n",
    "    df_train, df_test = session.table(exp_details.get(\"dataset\")).drop('ROW').random_split(weights=[0.9, 0.1], seed=0)\n",
    "    features = df_train.columns\n",
    "    features.remove(exp_details.get(\"target_column\"))\n",
    "    \n",
    "    # get features\n",
    "    data_schema = session.sql(f\"DESCRIBE TABLE {exp_details.get('dataset')}\").collect()\n",
    "    categorical_types = ['VARCHAR','CHAR','STRING','TEXT','BOOL']\n",
    "    categorical_features = []\n",
    "    \n",
    "    for row in data_schema:\n",
    "        for typ in categorical_types:\n",
    "            if typ in row['type'] and row['name']!=exp_details.get(\"target_column\"):\n",
    "                categorical_features.append(row['name'])\n",
    "                break\n",
    "    numerical_features = list(set(features) - set(categorical_features))\n",
    "    log_message(\"INFO\",f\"numerical_features:  {numerical_features}\")\n",
    "    log_message(\"INFO\",f\"categorical_features_oe: {categorical_features}\")\n",
    "    \n",
    "    \n",
    "    #pipeline steps \n",
    "    log_message(\"INFO\",\"Setting up preprocessing pipeline based on dataset\")\n",
    "    categorical_pp = {\n",
    "        'ohe': OneHotEncoder(input_cols=categorical_features, output_cols=categorical_features)\n",
    "    }\n",
    "    numerical_pp = {\n",
    "        'scaler': MinMaxScaler(input_cols=numerical_features, output_cols=numerical_features)\n",
    "    }\n",
    "    steps = [(key, categorical_pp[key]) for key in categorical_pp if categorical_features!=[]] + \\\n",
    "    [(key, numerical_pp[key]) for key in numerical_pp if numerical_features!=[]]\n",
    "    \n",
    "    \n",
    "    # Define a pipeline that does the preprocessing and training of \n",
    "    # dynamically import selected algorithms\n",
    "    for algorithm, hyperparam in exp_details.get(\"algo_details\").items():\n",
    "        algorithm = algorithm.rsplit('.', 1)\n",
    "        module = importlib.import_module(algorithm[0])\n",
    "        log_message(\"INFO\",f\"Running Algorithm {algorithm[1]}\")\n",
    "        attr = getattr(module, algorithm[1])\n",
    "        \n",
    "        pipe = Pipeline(steps=steps+[(\"algorithm\", attr(input_cols=categorical_features+numerical_features\n",
    "                                              , label_cols=[exp_details.get(\"target_column\")]\n",
    "                                              , output_cols=[f'PREDICTIONS_{algorithm[1]}'.upper()]))]\n",
    "               )\n",
    "\n",
    "        # Fit the pipeline\n",
    "        log_message(\"INFO\",f\"Running model pipeline {algorithm[1]}\")\n",
    "        model = pipe.fit(df_train)\n",
    "        \n",
    "#         log_message(\"INFO\",f\"final model size {model.size()} bytes\")\n",
    "        \n",
    "        # Test the model\n",
    "        log_message(\"INFO\",\"Running prediction on model with test dataset\")\n",
    "        df_test_pred = model.predict(df_test)\n",
    "        \n",
    "        # metrices\n",
    "        log_message(\"INFO\",\"Generating Metrices\")\n",
    "        accuracy = accuracy_score(df=df_test_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names=f'PREDICTIONS_{algorithm[1]}'.upper())\n",
    "        f1_score = f1_score(df=df_test_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names=f'PREDICTIONS_{algorithm[1]}'.upper())\n",
    "        recall_score = recall_score(df=df_test_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names=f'PREDICTIONS_{algorithm[1]}'.upper())\n",
    "        precision_score = precision_score(df=df_test_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names=f'PREDICTIONS_{algorithm[1]}'.upper())\n",
    "        roc_auc_score = roc_auc_score(df=df_test_pred, y_true_col_names=exp_details.get(\"target_column\"), y_score_col_names=f'PREDICTIONS_{algorithm[1]}'.upper())\n",
    "        print(\"Execution Completed\")\n",
    "        print(f'{algorithm[1]} MSE: {accuracy}')\n",
    "\n",
    "        print(f'{algorithm[1]} R2: {precision_score}')\n",
    "        print(f'{algorithm[1]} R2: {roc_auc_score}')\n",
    "        \n",
    "\n",
    "        # LOG MODEL INTO SNOWFLAKE REGISTRY\n",
    "        from snowflake.ml.registry.registry import Registry\n",
    "        reg = Registry(session=session)\n",
    "        # Log the model\n",
    "        log_message(\"INFO\",\"Started: Registering model on snowflake\")\n",
    "        try:\n",
    "            mv = reg.log_model(model=model,\n",
    "                               model_name=exp_details.get(\"name\", \"sample_experiment\")+\"_\"+algorithm[1],\n",
    "                               comment=\"test\",\n",
    "                               version_name=\"run1\",\n",
    "                               python_version=\"3.9.19\",\n",
    "                               conda_dependencies=[\"scikit-learn==1.3.2\"],\n",
    "                               metrics=[{\"model_metrics\": {\"roc_auc_score\": roc_auc_score, \"precision_score\": precision_score, \"f1_score\": f1_score, \"recall_score\": recall_score, \"accuracy_score\": accuracy}, \"project_id\": \"0001\", \"type\": \"EXP\"}])\n",
    "            log_message(\"INFO\",\"Registeration of model completed!!!\")\n",
    "        except Exception as ex:\n",
    "            key = 'Processing aborted due to error 370001' \n",
    "            if key in str(ex):\n",
    "                log_message(\"INFO\",\"Registeration of model completed!!!\")\n",
    "                pass\n",
    "            else:\n",
    "                log_message(\"ERROR\",\"Exception Occured while registering model\")\n",
    "                return str(ex).split('?')\n",
    "    return [{\"Execution Logs:\": \"\\n\".join(logs),\n",
    "             \"EXP_NAME\":exp_details.get(\"name\", \"sample_experiment\"),\n",
    "             \"Version\":\"Run1\",\n",
    "             \"matrices\":{\"model_metrics\": {\"roc_auc_score\": roc_auc_score, \"precision_score\": precision_score, \"f1_score\": f1_score, \"recall_score\": recall_score, \"accuracy_score\": accuracy}, \"project_id\": \"0001\", \"type\": \"EXP\"},\n",
    "             \"Alogirthm_Type\":\"Regression\",\n",
    "             \"Alogithms\": list(exp_details.get(\"algo_details\").keys()),\n",
    "             \"RUN_STATUS\":\"SUCCESS\",\n",
    "             \"registry_exp_name\":\"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilization\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print(\"Creating Snowflake Session object...\")\n",
    "session = get_session()\n",
    "stage = create_stage(session)\n",
    "print(\"Session has been created !\")\n",
    "\n",
    "print(\"Creating stored procedure...\")\n",
    "session.sproc.register(func=train_ml_models,\n",
    "                       name=\"train_ml_models\",\n",
    "                       packages=[\"snowflake-snowpark-python\", \"snowflake-ml-python\"],\n",
    "                       isPermanant=False,\n",
    "                       stage_location=stage,\n",
    "                       replace=True)\n",
    "print(\"Stored procedure has been created successfully!\")\n",
    "\n",
    "print(\"Executing Procedure\")\n",
    "procedure_response = session.call(\"train_ml_models\", exp_data)\n",
    "# procedure_response = train_ml_models(session, exp_data)\n",
    "print(\"Stored Procedure Executed Successfully !\")\n",
    "print(procedure_response)\n",
    "\n",
    "#Log in mlflow\n",
    "print(\"Logging in mlflow completed !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
