{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9345492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3ccc4ba4-349b-4e6d-8745-d8008966665b\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "# project_id = str(uuid.uuid4())\n",
    "run_id = str(uuid.uuid4())\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d621e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-1 (classification Airline Delay dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"classification\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.xgboost.XGBClassifier\": null}}, \n",
    "\"dataset\": \"AIRLINE_DEP_DELAY_10K\", \n",
    "\"target_column\": \"DEP_DEL15\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df37eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-2 (Regression Alcohol dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"regression\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.ensemble.RandomForestRegressor\": null}}, \n",
    "\"dataset\": \"ALCOHOL_DATA_10L\", \n",
    "\"target_column\": \"QUALITY\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-3 (classification Employee dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"classification\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.linear_model.LogisticRegression\": null}}, \n",
    "\"dataset\": \"EMPLOYEE_10L\", \n",
    "\"target_column\": \"LEAVEORNOT\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# payload-4 (Regression Diamonds dataset)\n",
    "exp_data = '''{{\n",
    "\"source\":\"EXP\", \n",
    "\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \n",
    "\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \n",
    "\"run_id\":\"{0}\", \n",
    "\"exp_name\": \"Final_recipe\", \n",
    "\"algorithm_type\":\"regression\", \n",
    "\"algo_details\": {{\"snowflake.ml.modeling.xgboost.XGBRegressor\": null}}, \n",
    "\"dataset\": \"DIAMONDS\", \n",
    "\"target_column\": \"PRICE\"}}'''.format(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8fc1b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\"source\":\"EXP\", \\n\"project_id\":\"0e0fb803-22db-4d88-9f2f-f6f75b6abcf0\", \\n\"id\":\"7bbb5061-54d4-4862-8d47-7fbee388a4d1\", \\n\"run_id\":\"3ccc4ba4-349b-4e6d-8745-d8008966665b\", \\n\"exp_name\": \"Final_recipe\", \\n\"algorithm_type\":\"classification\", \\n\"algo_details\": {\"snowflake.ml.modeling.xgboost.XGBClassifier\": null}, \\n\"dataset\": \"AIRLINE_DEP_DELAY_10K\", \\n\"target_column\": \"DEP_DEL15\"}'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "101789b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection manager service url initialised to http://fdc-project-manager:80/project-manager\n",
      "If you need to update its value then update the variable CONNECTION_MANAGER_BASE_URL in os env.\n"
     ]
    }
   ],
   "source": [
    "from fosforio.manager import get_conn_details_from_ds_name\n",
    "from snowflake.snowpark.session import Session\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50e7bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage(session, stage_name=\"demo\"):\n",
    "    try:\n",
    "        session.sql(f\"create or replace stage {stage_name}\").collect()\n",
    "        return f\"@{stage_name}\"\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake session\", ex)\n",
    "        raise ex\n",
    "\n",
    "        \n",
    "def get_session(dataset, project_id):\n",
    "    \"\"\"\n",
    "    Method creates snowflake session object.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_conn_details_from_ds_name(dataset, project_id)\n",
    "        region=conn[\"params\"][\"READER\"][\"region\"] if conn[\"params\"][\"READER\"][\"cloudPlatform\"] is None \\\n",
    "                    else conn[\"params\"][\"READER\"][\"region\"]+\".\"+conn[\"params\"][\"READER\"][\"cloudPlatform\"]\n",
    "        account = conn['params']['READER']['accountId'] if region is None \\\n",
    "                    else conn['params']['READER']['accountId']+\".\"+region\n",
    "        CONNECTION_PARAMETERS = {\n",
    "            \"account\": account,\n",
    "            \"user\":conn['params']['READER']['user'],\n",
    "            \"password\": conn['params']['READER']['password'],\n",
    "            \"role\": conn['params']['READER']['role'],\n",
    "            \"database\": conn['params']['READER']['database'],\n",
    "            \"warehouse\": conn['params']['READER']['wareHouse'],\n",
    "            \"schema\": conn['params']['READER']['schema']\n",
    "        }\n",
    "        return Session.builder.configs(CONNECTION_PARAMETERS).create()\n",
    "    except Exception as ex:\n",
    "        print(\"Error while creating snowflake session\", ex)\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec151ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stored Procedure \n",
    "def run_experiment(session: Session, exp_data: str) -> list:\n",
    "    # Imports\n",
    "    from snowflake.ml.modeling.pipeline import Pipeline\n",
    "    from snowflake.ml.modeling.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "    from snowflake.ml.modeling.metrics import r2_score, accuracy_score, precision_score, roc_auc_score, \\\n",
    "        f1_score, recall_score, log_loss, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    from snowflake.snowpark.functions import col, is_null, regexp_replace, when, lit\n",
    "    from snowflake.snowpark.types import StringType\n",
    "    from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "    import importlib, sys, json, os, logging\n",
    "    from snowflake.ml.registry.registry import Registry\n",
    "    \n",
    "    \n",
    "    # Functions used in stored proc\n",
    "    def apply_data_cleansing(df):\n",
    "        \"\"\"\n",
    "        Method handles null values in snowpark dataframe.\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        :returns:\n",
    "        df_cleaned: dataframe after null handling\n",
    "        \"\"\"\n",
    "        # fillna - Unknown and 0\n",
    "        schema_fields = df.schema.fields\n",
    "        fill_values = {field.name: \"UNKNOWN\" if isinstance(field.datatype, StringType) else 0 for field in schema_fields}\n",
    "        df_cleaned = df.fillna(fill_values)\n",
    "        return df_cleaned\n",
    "\n",
    "\n",
    "    def get_feature_columns(df):\n",
    "        \"\"\"\n",
    "        Identifies the numerical and categorical features in dataset.\n",
    "        Identifies features for label encoding and one hot encoding\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        :returns:\n",
    "        categorical_features: list of non-numerical feature columns\n",
    "        numerical_features: list of numerical feature columns\n",
    "        le_column_features: list of feature label encoder columns\n",
    "        oh_column_features: list of feature one hot encoder columns\n",
    "        \"\"\"\n",
    "        schema_fields = df.schema.fields\n",
    "        features = df.columns\n",
    "        features.remove(exp_details.get(\"target_column\"))\n",
    "        df_schema = session.sql(f\"DESCRIBE TABLE {exp_details.get('dataset')}\").collect()\n",
    "        categorical_types = ['VARCHAR','CHAR','STRING','TEXT','BOOL']\n",
    "        categorical_features = []  \n",
    "        for row in df_schema:\n",
    "            for typ in categorical_types:\n",
    "                if typ in row['type']:\n",
    "                    categorical_features.append(row['name'])\n",
    "                    break\n",
    "        numerical_features = list(set(features) - set(categorical_features))\n",
    "        print(f\"numerical_features:  {numerical_features}\")\n",
    "        print(f\"categorical_features: {categorical_features}\")\n",
    "        \n",
    "        #identify columns for labelencoding and onehotencoding   \n",
    "        le_column_features = categorical_features\n",
    "        oh_column_features = []\n",
    "        if len(categorical_features) >= 1:\n",
    "            print(f\"{categorical_features} columns are non numeric in feature dataset, encoding required.\")\n",
    "            for column in categorical_features:\n",
    "                if df.select(df[column]).distinct().count() < 10:\n",
    "                    oh_column_features.append(column)\n",
    "            print(f\"Columns identified to be encoded with label encoder: {le_column_features}\")\n",
    "            print(f\"Columns identified to be encoded with one hot encoder: {oh_column_features}\")\n",
    "        return categorical_features, numerical_features, le_column_features, oh_column_features\n",
    "\n",
    "\n",
    "    def create_and_run_preprocessing(df, categorical_features, numerical_features, le_column_features, oh_column_features):\n",
    "        \"\"\"\n",
    "        Based on different features column input creates preprocessing steps and run it on input dataframe\n",
    "        :param:\n",
    "        df: input dataframe\n",
    "        categorical_features: list of non-numerical feature columns\n",
    "        numerical_features: list of numerical feature columns\n",
    "        le_column_features: list of feature label encoder columns\n",
    "        oh_column_features: list of feature one hot encoder columns\n",
    "        :returns:\n",
    "        df_train: preprocessed train dataset\n",
    "        df_test: preprocessed test dataset\n",
    "        \"\"\"\n",
    "        #pipeline steps \n",
    "        print(\"Setting up preprocessing pipeline based on dataset\")\n",
    "        categorical_pp = {f'le_{column}':LabelEncoder(input_cols=column, output_cols=column) for column in le_column_features}\n",
    "        if len(oh_column_features)>0:\n",
    "            categorical_pp['oh_enc'] = OneHotEncoder(input_cols=oh_column_features, output_cols=oh_column_features, handle_unknown='ignore')\n",
    "        numerical_pp = {\n",
    "            'scaler': MinMaxScaler(input_cols=numerical_features, output_cols=numerical_features)\n",
    "        }\n",
    "        steps = [(key, categorical_pp[key]) for key in categorical_pp if categorical_pp[key]!=[]] + \\\n",
    "        [(key, numerical_pp[key]) for key in numerical_pp if numerical_features!=[]]\n",
    "        print(f\"Selected preprocesing steps: \\n{steps}\")   \n",
    "        \n",
    "        return steps\n",
    "            \n",
    "        # Run preprocessing pipeline steps \n",
    "#         print(\"Running data preprocessing pipeline\")\n",
    "#         df = Pipeline(steps=steps).fit(df).transform(df)\n",
    "#         print(f\"Transformed dataset: \\n {df.show()}\")\n",
    "#         df_train, df_test = df.random_split(weights=[0.8, 0.2], seed=0)\n",
    "#         return df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "    def run_estimator(df, steps, input_cols):\n",
    "        \"\"\"\n",
    "        trains on df_train and creates model object for given algorithm/estimator.\n",
    "        runs prediction function of model object on test dataset\n",
    "        :param:\n",
    "        df_train: input training dataframe\n",
    "        df_test: input test dataframe\n",
    "        input_cols: list of input feature names\n",
    "        :returns:\n",
    "        df_pred: output predict dataframe\n",
    "        \"\"\"\n",
    "        df_train, df_test = df.random_split(weights=[0.8, 0.2], seed=0)\n",
    "        for algorithm, hyperparam in exp_details.get(\"algo_details\").items():\n",
    "            algorithm = algorithm.rsplit('.', 1)\n",
    "            module = importlib.import_module(algorithm[0])\n",
    "            print(f\"----Running Algorithm {algorithm[1]}----\")\n",
    "            attr = getattr(module, algorithm[1])\n",
    "            pipe = Pipeline(steps=steps+[(\"algorithm\", attr(input_cols=input_cols\n",
    "                                                  , label_cols=[exp_details.get(\"target_column\")]\n",
    "                                                  , output_cols=['PREDICTIONS']))]\n",
    "                   )\n",
    "    \n",
    "            # Fit the pipeline\n",
    "            print(f\"Running model pipeline {algorithm[1]}\")\n",
    "            model = pipe.fit(df_train)\n",
    "     \n",
    "            # Test the model\n",
    "            print(\"Running prediction on model with test dataset\")\n",
    "            df_pred = model.predict(df_test)\n",
    "            return model, df_pred\n",
    "\n",
    "    \n",
    "    def try_or(fn):\n",
    "        try:\n",
    "            out = fn()\n",
    "            return out\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "        \n",
    "    def eval_metrics(df_pred):\n",
    "        print(\"Generating Metrices\")\n",
    "        if exp_details.get(\"algorithm_type\") == 'classification':\n",
    "            return {\n",
    "            'accuracy_score': try_or(lambda: accuracy_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'f1_score': try_or(lambda: f1_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'recall_score': try_or(lambda: recall_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'precision_score': try_or(lambda: precision_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'roc_auc': try_or(lambda: roc_auc_score(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_score_col_names='PREDICTIONS')),\n",
    "            'log_loss': try_or(lambda: log_loss(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS'))\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "            'mean_absolute_percentage_error': try_or(lambda: mean_absolute_percentage_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'r2_score': try_or(lambda: r2_score(df=df_pred, y_true_col_name=exp_details.get(\"target_column\"), y_pred_col_name='PREDICTIONS')),\n",
    "            'mean_absolute_error': try_or(lambda: mean_absolute_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS')),\n",
    "            'mean_squared_error': try_or(lambda: mean_squared_error(df=df_pred, y_true_col_names=exp_details.get(\"target_column\"), y_pred_col_names='PREDICTIONS'))\n",
    "            }\n",
    "\n",
    "\n",
    "    def register_model(model, metrics_info):\n",
    "        print(\"Started: Registering model on snowflake\")\n",
    "        reg = Registry(session=session)\n",
    "        \n",
    "        clean = lambda x : x.replace(\"-\",\"_\")\n",
    "        project_id = clean(exp_details.get(\"project_id\"))\n",
    "        exp_id = clean(exp_details.get(\"id\"))\n",
    "        run_id = clean(exp_details.get(\"run_id\"))\n",
    "        model_name = exp_details.get(\"exp_name\")+\"_\"+project_id+\"_\"+exp_id+\"_\"+run_id\n",
    "        mv = reg.log_model(model=model,\n",
    "                           model_name=model_name,\n",
    "                           comment=exp_details.get(\"description\"),\n",
    "                           version_name=\"V1\",\n",
    "                           python_version=\"3.9.19\",\n",
    "                           conda_dependencies=[\"xgboost\",\"scikit-learn\"],\n",
    "                           metrics=[{\"model_metrics\": metrics_info, \n",
    "                                     \"project_id\": exp_details.get(\"project_id\"),\n",
    "                                     \"id\": exp_details.get(\"id\"),\n",
    "                                     \"run_id\": exp_details.get(\"run_id\"),\n",
    "                                     \"algorithm_type\": exp_details.get(\"algorithm_type\"),\n",
    "                                     \"algo_details\": exp_details.get(\"algo_details\"),\n",
    "                                     \"dataset\": exp_details.get(\"dataset\"),\n",
    "                                     \"target_column\": exp_details.get(\"target_column\"),\n",
    "                                     \"exp_name\": exp_details.get(\"exp_name\"),\n",
    "                                     \"source\": \"EXP\"}])\n",
    "        print(\"Registeration of model completed!!!\")\n",
    "        return model_name\n",
    "            \n",
    "            \n",
    "    def create_tags(session, exp_details):\n",
    "        for key in exp_details.keys():\n",
    "            tag = session.sql(f\"CREATE TAG IF NOT EXISTS {key}\")\n",
    "            tag.show()\n",
    "        if exp_details.get('algorithm_type')=='classification':\n",
    "            metric_names=[\"accuracy_score\",\"precision_score\",\"recall_score\",\"f1_score\",\"roc_auc\",\"log_loss\"]\n",
    "        else:\n",
    "            metric_names=[\"r2_score\",\"mean_absolute_error\",\"mean_squared_error\",\"mean_absolute_percentage_error\"]\n",
    "        for name in metric_names:\n",
    "            tag = session.sql(f\"CREATE TAG IF NOT EXISTS {name}\")\n",
    "            tag.show()\n",
    "\n",
    "        \n",
    "    def set_tags(session, m_name, exp_details, metric_info):\n",
    "        for key, value in exp_details.items():\n",
    "            value = str(value).replace(\"'\",\"\\\"\")\n",
    "            tag = session.sql(f\"ALTER MODEL IF EXISTS {m_name} SET TAG {key}='{value}'\")\n",
    "            tag.show()\n",
    "        for key, value in metric_info.items():\n",
    "            value = str(value).replace(\"'\",\"\\\"\")\n",
    "            tag = session.sql(f\"ALTER MODEL IF EXISTS {m_name} SET TAG {key}='{value}'\")\n",
    "            tag.show()\n",
    "    \n",
    "#     try:\n",
    "    # loading experiment details\n",
    "    exp_details=json.loads(exp_data)\n",
    "    \n",
    "    # creating user tags if not exist\n",
    "    create_tags(session, exp_details)\n",
    "    \n",
    "    # Reading dataset\n",
    "    print(\"Reading dataset features\")\n",
    "    data = session.table(exp_details.get(\"dataset\"))\n",
    "    \n",
    "    # fillna\n",
    "    data = apply_data_cleansing(data)\n",
    "    \n",
    "    # Identify feature columns\n",
    "    categorical_features, numerical_features, le_column_features, oh_column_features = get_feature_columns(data)\n",
    "    \n",
    "    # Based on feature, select preprocessing steps\n",
    "    pp_steps = create_and_run_preprocessing(data, categorical_features, numerical_features, le_column_features, oh_column_features)\n",
    "    \n",
    "    # Run model training and prediction\n",
    "    input_cols = data.columns\n",
    "    input_cols.remove(exp_details.get(\"target_column\"))    \n",
    "    model, data_pred = run_estimator(data, pp_steps, input_cols)\n",
    "    \n",
    "    # Evaluate model metrices\n",
    "    metrics_info = eval_metrics(data_pred)\n",
    "    print(metrics_info)\n",
    "    \n",
    "    # Register model on snowflake registry\n",
    "    model_name = register_model(model, metrics_info)\n",
    "    print(model_name)\n",
    "    \n",
    "    # Set relevant tags to model object\n",
    "    set_tags(session, model_name, exp_details, metrics_info)\n",
    "    \n",
    "    return [{\"exp_name\":exp_details.get(\"exp_name\", \"sample_experiment\"),\n",
    "         \"version\":\"V1\",\n",
    "         \"matrices\":{\"model_metrics\": metrics_info, \"project_id\": exp_details.get(\"project_id\"), \"source\": \"EXP\"},\n",
    "         \"algorithm_type\":exp_details.get(\"algorithm_type\"),\n",
    "         \"algorithm\": list(exp_details.get(\"algo_details\").keys()),\n",
    "         \"dataset\": exp_details.get(\"dataset\"),\n",
    "         \"target_column\": exp_details.get(\"target_column\"),\n",
    "         \"RUN_STATUS\": \"SUCCESS\",\n",
    "         \"registry_model_name\":exp_details.get(\"exp_name\")+\"_\"+exp_details.get(\"project_id\")+\"_\"+exp_details.get(\"id\")+\"_\"+exp_details.get(\"run_id\")}]\n",
    "#     except Exception as ex:\n",
    "#         key = 'Processing aborted due to error 370001' \n",
    "#         if key in str(ex):\n",
    "#             print(\"Registeration of model completed!!!\")\n",
    "#             return model_name\n",
    "#         else:\n",
    "#             print(\"Exception Occured in run experiment\")\n",
    "#             return str(ex).split('?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "914d9fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Snowflake Session object...\n",
      "Session has been created !\n",
      "Creating stored procedure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package 'snowflake-snowpark-python' in the local environment is 1.19.0, which does not fit the criteria for the requirement 'snowflake-snowpark-python'. Your UDF might not work when the package version is different between the server and your local environment.\n",
      "The version of package 'snowflake-ml-python' in the local environment is 1.5.3, which does not fit the criteria for the requirement 'snowflake-ml-python'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored procedure has been created successfully!\n",
      "Setting tag to session object: tag=  3ccc4ba4-349b-4e6d-8745-d8008966665b\n",
      "Executing Procedure\n",
      "-----------------------------------------------\n",
      "|\"status\"                                     |\n",
      "-----------------------------------------------\n",
      "|SOURCE already exists, statement succeeded.  |\n",
      "-----------------------------------------------\n",
      "\n",
      "---------------------------------------------------\n",
      "|\"status\"                                         |\n",
      "---------------------------------------------------\n",
      "|PROJECT_ID already exists, statement succeeded.  |\n",
      "---------------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "|\"status\"                                 |\n",
      "-------------------------------------------\n",
      "|ID already exists, statement succeeded.  |\n",
      "-------------------------------------------\n",
      "\n",
      "-----------------------------------------------\n",
      "|\"status\"                                     |\n",
      "-----------------------------------------------\n",
      "|RUN_ID already exists, statement succeeded.  |\n",
      "-----------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|\"status\"                                       |\n",
      "-------------------------------------------------\n",
      "|EXP_NAME already exists, statement succeeded.  |\n",
      "-------------------------------------------------\n",
      "\n",
      "------------------------------------------------------\n",
      "|\"status\"                                            |\n",
      "------------------------------------------------------\n",
      "|ALGORITHM_TYPE already exists, statement succee...  |\n",
      "------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "|\"status\"                                           |\n",
      "-----------------------------------------------------\n",
      "|ALGO_DETAILS already exists, statement succeeded.  |\n",
      "-----------------------------------------------------\n",
      "\n",
      "------------------------------------------------\n",
      "|\"status\"                                      |\n",
      "------------------------------------------------\n",
      "|DATASET already exists, statement succeeded.  |\n",
      "------------------------------------------------\n",
      "\n",
      "------------------------------------------------------\n",
      "|\"status\"                                            |\n",
      "------------------------------------------------------\n",
      "|TARGET_COLUMN already exists, statement succeeded.  |\n",
      "------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------\n",
      "|\"status\"                                            |\n",
      "------------------------------------------------------\n",
      "|ACCURACY_SCORE already exists, statement succee...  |\n",
      "------------------------------------------------------\n",
      "\n",
      "------------------------------------------------------\n",
      "|\"status\"                                            |\n",
      "------------------------------------------------------\n",
      "|PRECISION_SCORE already exists, statement succe...  |\n",
      "------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------\n",
      "|\"status\"                                           |\n",
      "-----------------------------------------------------\n",
      "|RECALL_SCORE already exists, statement succeeded.  |\n",
      "-----------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|\"status\"                                       |\n",
      "-------------------------------------------------\n",
      "|F1_SCORE already exists, statement succeeded.  |\n",
      "-------------------------------------------------\n",
      "\n",
      "------------------------------------------------\n",
      "|\"status\"                                      |\n",
      "------------------------------------------------\n",
      "|ROC_AUC already exists, statement succeeded.  |\n",
      "------------------------------------------------\n",
      "\n",
      "-------------------------------------------------\n",
      "|\"status\"                                       |\n",
      "-------------------------------------------------\n",
      "|LOG_LOSS already exists, statement succeeded.  |\n",
      "-------------------------------------------------\n",
      "\n",
      "Reading dataset features\n",
      "numerical_features:  ['MONTH', 'DAY_OF_WEEK', 'DEP_AIRPORT_HIST', 'AWND', 'CONCURRENT_FLIGHTS', 'FLT_ATTENDANTS_PER_PASS', 'CARRIER_HISTORICAL', 'SNWD', 'LATITUDE', 'DISTANCE_GROUP', 'AIRLINE_AIRPORT_FLIGHTS_MONTH', 'DEP_BLOCK_HIST', 'AVG_MONTHLY_PASS_AIRLINE', 'TMAX', 'GROUND_SERV_PER_PASS', 'NUMBER_OF_SEATS', 'SEGMENT_NUMBER', 'LONGITUDE', 'PRCP', 'PLANE_AGE', 'SNOW', 'AIRPORT_FLIGHTS_MONTH', 'AIRLINE_FLIGHTS_MONTH', 'AVG_MONTHLY_PASS_AIRPORT', 'DAY_HISTORICAL']\n",
      "categorical_features: ['DEP_TIME_BLK', 'CARRIER_NAME', 'DEPARTING_AIRPORT', 'PREVIOUS_AIRPORT']\n",
      "['DEP_TIME_BLK', 'CARRIER_NAME', 'DEPARTING_AIRPORT', 'PREVIOUS_AIRPORT'] columns are non numeric in feature dataset, encoding required.\n",
      "Columns identified to be encoded with label encoder: ['DEP_TIME_BLK', 'CARRIER_NAME', 'DEPARTING_AIRPORT', 'PREVIOUS_AIRPORT']\n",
      "Columns identified to be encoded with one hot encoder: []\n",
      "Setting up preprocessing pipeline based on dataset\n",
      "Selected preprocesing steps: \n",
      "[('le_DEP_TIME_BLK', <snowflake.ml.modeling.preprocessing.label_encoder.LabelEncoder object at 0x7fccf85114c0>), ('le_CARRIER_NAME', <snowflake.ml.modeling.preprocessing.label_encoder.LabelEncoder object at 0x7fccf8511c40>), ('le_DEPARTING_AIRPORT', <snowflake.ml.modeling.preprocessing.label_encoder.LabelEncoder object at 0x7fccf8511d90>), ('le_PREVIOUS_AIRPORT', <snowflake.ml.modeling.preprocessing.label_encoder.LabelEncoder object at 0x7fccf85119d0>), ('scaler', <snowflake.ml.modeling.preprocessing.min_max_scaler.MinMaxScaler object at 0x7fccf8511f70>)]\n",
      "----Running Algorithm XGBClassifier----\n",
      "Running model pipeline XGBClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package 'snowflake-snowpark-python' in the local environment is 1.19.0, which does not fit the criteria for the requirement 'snowflake-snowpark-python'. Your UDF might not work when the package version is different between the server and your local environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction on model with test dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "(2112) Found unknown categories during transform:\n        COLUMN_NAME  UNKNOWN_VALUE\n0  PREVIOUS_AIRPORT            NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSnowflakeMLException\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/_internal/telemetry.py:368\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/modeling/preprocessing/ordinal_encoder.py:463\u001b[0m, in \u001b[0;36mOrdinalEncoder.transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, snowpark\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m--> 463\u001b[0m     output_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_snowpark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/modeling/preprocessing/ordinal_encoder.py:538\u001b[0m, in \u001b[0;36mOrdinalEncoder._transform_snowpark\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    536\u001b[0m     transformed_dataset \u001b[38;5;241m=\u001b[39m transformed_dataset\u001b[38;5;241m.\u001b[39mwith_column_renamed(F\u001b[38;5;241m.\u001b[39mcol(_CATEGORY \u001b[38;5;241m+\u001b[39m suffix), _CATEGORY)\n\u001b[0;32m--> 538\u001b[0m transformed_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_unknown_in_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# Reorder columns. Passthrough columns are added at the right to the output of the transformers.\u001b[39;00m\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/modeling/preprocessing/ordinal_encoder.py:630\u001b[0m, in \u001b[0;36mOrdinalEncoder._handle_unknown_in_transform\u001b[0;34m(self, transformed_dataset)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# batch columns to avoid query compilation OOM\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_unknown\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformed_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_COLUMN_BATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtelemetry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_statement_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPROJECT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSUBPROJECT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_encoded_value\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# left outer join has already filled unknown values with null\u001b[39;00m\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/modeling/preprocessing/ordinal_encoder.py:726\u001b[0m, in \u001b[0;36mOrdinalEncoder._check_unknown\u001b[0;34m(self, dataset, statement_params, batch)\u001b[0m\n\u001b[1;32m    725\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound unknown categories during transform:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mconcat_unknown_pandas\u001b[38;5;241m.\u001b[39mto_string()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mSnowflakeMLException(\n\u001b[1;32m    727\u001b[0m     error_code\u001b[38;5;241m=\u001b[39merror_codes\u001b[38;5;241m.\u001b[39mINVALID_DATA,\n\u001b[1;32m    728\u001b[0m     original_exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mValueError\u001b[39;00m(msg),\n\u001b[1;32m    729\u001b[0m )\n",
      "\u001b[0;31mSnowflakeMLException\u001b[0m: ValueError('(2112) Found unknown categories during transform:\\n        COLUMN_NAME  UNKNOWN_VALUE\\n0  PREVIOUS_AIRPORT            NaN')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:25\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 242\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(session, exp_data)\u001b[0m\n\u001b[1;32m    240\u001b[0m input_cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m    241\u001b[0m input_cols\u001b[38;5;241m.\u001b[39mremove(exp_details\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_column\u001b[39m\u001b[38;5;124m\"\u001b[39m))    \n\u001b[0;32m--> 242\u001b[0m model, data_pred \u001b[38;5;241m=\u001b[39m \u001b[43mrun_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpp_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Evaluate model metrices\u001b[39;00m\n\u001b[1;32m    245\u001b[0m metrics_info \u001b[38;5;241m=\u001b[39m eval_metrics(data_pred)\n",
      "Cell \u001b[0;32mIn[12], line 135\u001b[0m, in \u001b[0;36mrun_experiment.<locals>.run_estimator\u001b[0;34m(df, steps, input_cols)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning prediction on model with test dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m df_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, df_pred\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/_internal/telemetry.py:373\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, snowml_exceptions\u001b[38;5;241m.\u001b[39mSnowflakeMLException):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# already handled via a nested decorated function\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_snowflake_ml_handled\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39m_snowflake_ml_handled:\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, snowpark_exceptions\u001b[38;5;241m.\u001b[39mSnowparkClientException):\n\u001b[1;32m    375\u001b[0m         me \u001b[38;5;241m=\u001b[39m snowml_exceptions\u001b[38;5;241m.\u001b[39mSnowflakeMLException(\n\u001b[1;32m    376\u001b[0m             error_code\u001b[38;5;241m=\u001b[39merror_codes\u001b[38;5;241m.\u001b[39mINTERNAL_SNOWPARK_ERROR, original_exception\u001b[38;5;241m=\u001b[39me\n\u001b[1;32m    377\u001b[0m         )\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/_internal/telemetry.py:368\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m telemetry_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    360\u001b[0m     func_name\u001b[38;5;241m=\u001b[39m_get_full_func_name(func),\n\u001b[1;32m    361\u001b[0m     function_category\u001b[38;5;241m=\u001b[39mTelemetryField\u001b[38;5;241m.\u001b[39mFUNC_CAT_USAGE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     custom_tags\u001b[38;5;241m=\u001b[39mcustom_tags,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, snowml_exceptions\u001b[38;5;241m.\u001b[39mSnowflakeMLException):\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;66;03m# already handled via a nested decorated function\u001b[39;00m\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/modeling/pipeline/pipeline.py:635\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handler\u001b[38;5;241m.\u001b[39mbatch_inference(\n\u001b[1;32m    627\u001b[0m         inference_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    628\u001b[0m         input_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_cols \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_cols \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_input_cols(dataset),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    631\u001b[0m         dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deps,\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke_estimator_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/modeling/pipeline/pipeline.py:827\u001b[0m, in \u001b[0;36mPipeline._invoke_estimator_func\u001b[0;34m(self, func_name, dataset)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_fitted:\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mSnowflakeMLException(\n\u001b[1;32m    823\u001b[0m         error_code\u001b[38;5;241m=\u001b[39merror_codes\u001b[38;5;241m.\u001b[39mMETHOD_NOT_ALLOWED,\n\u001b[1;32m    824\u001b[0m         original_exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline is not fitted before calling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m().\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    825\u001b[0m     )\n\u001b[0;32m--> 827\u001b[0m transformed_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_estimator()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/modeling/pipeline/pipeline.py:248\u001b[0m, in \u001b[0;36mPipeline._transform_dataset\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    246\u001b[0m transformed_dataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, trans \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_transformers():\n\u001b[0;32m--> 248\u001b[0m     transformed_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_dataset\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/_internal/telemetry.py:373\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, snowml_exceptions\u001b[38;5;241m.\u001b[39mSnowflakeMLException):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# already handled via a nested decorated function\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_snowflake_ml_handled\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39m_snowflake_ml_handled:\n\u001b[0;32m--> 373\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, snowpark_exceptions\u001b[38;5;241m.\u001b[39mSnowparkClientException):\n\u001b[1;32m    375\u001b[0m         me \u001b[38;5;241m=\u001b[39m snowml_exceptions\u001b[38;5;241m.\u001b[39mSnowflakeMLException(\n\u001b[1;32m    376\u001b[0m             error_code\u001b[38;5;241m=\u001b[39merror_codes\u001b[38;5;241m.\u001b[39mINTERNAL_SNOWPARK_ERROR, original_exception\u001b[38;5;241m=\u001b[39me\n\u001b[1;32m    377\u001b[0m         )\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/_internal/telemetry.py:368\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m telemetry_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    360\u001b[0m     func_name\u001b[38;5;241m=\u001b[39m_get_full_func_name(func),\n\u001b[1;32m    361\u001b[0m     function_category\u001b[38;5;241m=\u001b[39mTelemetryField\u001b[38;5;241m.\u001b[39mFUNC_CAT_USAGE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     custom_tags\u001b[38;5;241m=\u001b[39mcustom_tags,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, snowml_exceptions\u001b[38;5;241m.\u001b[39mSnowflakeMLException):\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;66;03m# already handled via a nested decorated function\u001b[39;00m\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/modeling/preprocessing/label_encoder.py:155\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, snowpark\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# [SNOW-802691] Support for mypy type checking\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ordinal_encoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     output_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ordinal_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mna\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    158\u001b[0m         subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_cols,\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     output_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_sklearn(dataset)\n",
      "File \u001b[0;32m/packages/Python-3.9-Snowpark/7e0b365a-f7cb-4d2a-ba2a-e8ed02dbe50c/3.9/snowflake/ml/_internal/telemetry.py:390\u001b[0m, in \u001b[0;36msend_api_usage_telemetry.<locals>.decorator.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m me\u001b[38;5;241m.\u001b[39moriginal_exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m me\u001b[38;5;241m.\u001b[39moriginal_exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m update_stmt_params_if_snowpark_df(res, statement_params)\n",
      "\u001b[0;31mValueError\u001b[0m: (2112) Found unknown categories during transform:\n        COLUMN_NAME  UNKNOWN_VALUE\n0  PREVIOUS_AIRPORT            NaN"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initilization\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print(\"Creating Snowflake Session object...\")\n",
    "exp_details=json.loads(exp_data)\n",
    "session = get_session(exp_details.get(\"dataset\"),exp_details.get(\"project_id\"))\n",
    "stage = create_stage(session)\n",
    "print(\"Session has been created !\")\n",
    "\n",
    "print(\"Creating stored procedure...\")\n",
    "session.custom_package_usage_config['enabled'] = True\n",
    "session.sproc.register(func=run_experiment,\n",
    "                       name=\"run_experiment\",\n",
    "                       packages=[\"snowflake-snowpark-python\", \"snowflake-ml-python\",\"scikit-learn\"],\n",
    "                       isPermanant=False,\n",
    "                       stage_location=stage,\n",
    "                       replace=True)\n",
    "print(\"Stored procedure has been created successfully!\")\n",
    "\n",
    "# tagging session\n",
    "print(\"Setting tag to session object: tag= \", run_id)\n",
    "session.query_tag=run_id\n",
    "\n",
    "print(\"Executing Procedure\")\n",
    "# procedure_response = session.call(\"run_experiment\", exp_data)\n",
    "procedure_response = run_experiment(session, exp_data)\n",
    "print(\"Stored Procedure Executed Successfully !\")\n",
    "print(procedure_response)\n",
    "\n",
    "#Log in mlflow\n",
    "print(\"Logging in snowflake registry completed !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc91bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
